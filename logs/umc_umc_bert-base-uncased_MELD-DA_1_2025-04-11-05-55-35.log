2025-04-11 05:55:35,871 - ============================== Params ==============================
2025-04-11 05:55:35,871 - logger_name: umc_umc_bert-base-uncased_MELD-DA_1
2025-04-11 05:55:35,871 - dataset: MELD-DA
2025-04-11 05:55:35,872 - multimodal_method: umc
2025-04-11 05:55:35,872 - method: umc
2025-04-11 05:55:35,872 - text_backbone: bert-base-uncased
2025-04-11 05:55:35,872 - seed: 1
2025-04-11 05:55:35,872 - num_workers: 16
2025-04-11 05:55:35,872 - log_id: umc_umc_bert-base-uncased_MELD-DA_1_2025-04-11-05-55-35
2025-04-11 05:55:35,872 - gpu_id: 1
2025-04-11 05:55:35,872 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-11 05:55:35,872 - train: True
2025-04-11 05:55:35,872 - tune: True
2025-04-11 05:55:35,872 - save_model: True
2025-04-11 05:55:35,872 - save_results: True
2025-04-11 05:55:35,872 - log_path: logs
2025-04-11 05:55:35,872 - cache_path: cache
2025-04-11 05:55:35,872 - video_data_path: video_data
2025-04-11 05:55:35,872 - audio_data_path: audio_data
2025-04-11 05:55:35,872 - video_feats_path: swin_feats.pkl
2025-04-11 05:55:35,872 - audio_feats_path: wavlm_feats.pkl
2025-04-11 05:55:35,872 - results_path: results
2025-04-11 05:55:35,872 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA
2025-04-11 05:55:35,872 - model_path: models
2025-04-11 05:55:35,872 - config_file_name: umc_MELD-DA
2025-04-11 05:55:35,872 - results_file_name: results_umc.csv
2025-04-11 05:55:35,872 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-11 05:55:35,872 - text_seq_len: 70
2025-04-11 05:55:35,872 - video_seq_len: 250
2025-04-11 05:55:35,872 - audio_seq_len: 520
2025-04-11 05:55:35,872 - text_feat_dim: 768
2025-04-11 05:55:35,873 - video_feat_dim: 1024
2025-04-11 05:55:35,873 - audio_feat_dim: 768
2025-04-11 05:55:35,873 - num_labels: 12
2025-04-11 05:55:35,873 - num_train_examples: 7990
2025-04-11 05:55:35,873 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-11 05:55:35,873 - pretrain_batch_size: 128
2025-04-11 05:55:35,873 - train_batch_size: 128
2025-04-11 05:55:35,873 - eval_batch_size: 128
2025-04-11 05:55:35,873 - test_batch_size: 128
2025-04-11 05:55:35,873 - num_pretrain_epochs: [100]
2025-04-11 05:55:35,873 - num_train_epochs: [100]
2025-04-11 05:55:35,873 - pretrain: [True]
2025-04-11 05:55:35,873 - aligned_method: ctc
2025-04-11 05:55:35,873 - need_aligned: False
2025-04-11 05:55:35,873 - freeze_pretrain_bert_parameters: True
2025-04-11 05:55:35,873 - freeze_train_bert_parameters: True
2025-04-11 05:55:35,873 - pretrain_temperature: [0.2]
2025-04-11 05:55:35,873 - train_temperature_sup: [20]
2025-04-11 05:55:35,873 - train_temperature_unsup: [20]
2025-04-11 05:55:35,873 - activation: tanh
2025-04-11 05:55:35,873 - lr_pre: [5e-06]
2025-04-11 05:55:35,873 - lr: [0.0002]
2025-04-11 05:55:35,873 - delta: [0.05]
2025-04-11 05:55:35,873 - thres: [0.1]
2025-04-11 05:55:35,873 - topk: [5]
2025-04-11 05:55:35,873 - weight_decay: 0.01
2025-04-11 05:55:35,873 - feat_dim: 768
2025-04-11 05:55:35,873 - hidden_size: 768
2025-04-11 05:55:35,874 - grad_clip: [-1.0]
2025-04-11 05:55:35,874 - warmup_proportion: 0.5
2025-04-11 05:55:35,874 - hidden_dropout_prob: 0.1
2025-04-11 05:55:35,874 - weight: 1.0
2025-04-11 05:55:35,874 - loss_mode: rdrop
2025-04-11 05:55:35,874 - base_dim: [256]
2025-04-11 05:55:35,874 - nheads: 8
2025-04-11 05:55:35,874 - attn_dropout: 0.1
2025-04-11 05:55:35,874 - relu_dropout: 0.1
2025-04-11 05:55:35,874 - embed_dropout: 0.1
2025-04-11 05:55:35,874 - res_dropout: 0.0
2025-04-11 05:55:35,874 - attn_mask: True
2025-04-11 05:55:35,874 - encoder_layers_1: 1
2025-04-11 05:55:35,874 - fusion_act: tanh
2025-04-11 05:55:35,874 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_1
2025-04-11 05:55:35,874 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_1/models
2025-04-11 05:55:35,874 - ============================== End Params ==============================
2025-04-11 05:55:37,133 - Freeze all parameters but the last layer for efficiency
2025-04-11 05:55:37,177 - Pre-training start...
2025-04-11 05:57:16,482 - ***** Epoch: 1: Eval results *****
2025-04-11 05:57:16,484 -   train_loss = 5.9199568430582685
2025-04-11 05:58:55,527 - ***** Epoch: 2: Eval results *****
2025-04-11 05:58:55,527 -   train_loss = 5.919612543923514
2025-04-11 06:00:32,943 - ***** Epoch: 3: Eval results *****
2025-04-11 06:00:32,943 -   train_loss = 5.918047322167291
2025-04-11 06:02:05,766 - ***** Epoch: 4: Eval results *****
2025-04-11 06:02:05,766 -   train_loss = 5.915350081428649
2025-04-11 06:03:36,164 - ***** Epoch: 5: Eval results *****
2025-04-11 06:03:36,165 -   train_loss = 5.910544228932214
2025-04-11 06:05:06,620 - ***** Epoch: 6: Eval results *****
2025-04-11 06:05:06,621 -   train_loss = 5.904828291090708
2025-04-11 06:06:38,882 - ***** Epoch: 7: Eval results *****
2025-04-11 06:06:38,882 -   train_loss = 5.895393439701626
2025-04-11 06:08:10,599 - ***** Epoch: 8: Eval results *****
2025-04-11 06:08:10,599 -   train_loss = 5.88253644912962
2025-04-11 06:09:43,488 - ***** Epoch: 9: Eval results *****
2025-04-11 06:09:43,488 -   train_loss = 5.859418793330117
2025-04-11 06:11:13,457 - ***** Epoch: 10: Eval results *****
2025-04-11 06:11:13,458 -   train_loss = 5.821018960740831
2025-04-11 06:12:43,427 - ***** Epoch: 11: Eval results *****
2025-04-11 06:12:43,428 -   train_loss = 5.759263545747787
2025-04-11 06:14:12,966 - ***** Epoch: 12: Eval results *****
2025-04-11 06:14:12,966 -   train_loss = 5.651240621294294
2025-04-11 06:15:43,108 - ***** Epoch: 13: Eval results *****
2025-04-11 06:15:43,108 -   train_loss = 5.503253316122388
2025-04-11 06:17:13,591 - ***** Epoch: 14: Eval results *****
2025-04-11 06:17:13,592 -   train_loss = 5.346366367642841
2025-04-11 06:18:44,062 - ***** Epoch: 15: Eval results *****
2025-04-11 06:18:44,063 -   train_loss = 5.192315994747101
2025-04-11 06:20:14,137 - ***** Epoch: 16: Eval results *****
2025-04-11 06:20:14,137 -   train_loss = 5.043858111850799
2025-04-11 06:21:44,464 - ***** Epoch: 17: Eval results *****
2025-04-11 06:21:44,464 -   train_loss = 4.862625511865767
2025-04-11 06:23:18,565 - ***** Epoch: 18: Eval results *****
2025-04-11 06:23:18,566 -   train_loss = 4.677975495656331
2025-04-11 06:24:50,857 - ***** Epoch: 19: Eval results *****
2025-04-11 06:24:50,858 -   train_loss = 4.50116877707224
2025-04-11 06:26:24,959 - ***** Epoch: 20: Eval results *****
2025-04-11 06:26:24,960 -   train_loss = 4.341591373322502
2025-04-11 06:27:55,529 - ***** Epoch: 21: Eval results *****
2025-04-11 06:27:55,529 -   train_loss = 4.184575663672553
2025-04-11 06:29:24,876 - ***** Epoch: 22: Eval results *****
2025-04-11 06:29:24,877 -   train_loss = 4.055237365147424
2025-04-11 06:30:54,679 - ***** Epoch: 23: Eval results *****
2025-04-11 06:30:54,680 -   train_loss = 3.936538049152919
2025-04-11 06:32:24,774 - ***** Epoch: 24: Eval results *****
2025-04-11 06:32:24,775 -   train_loss = 3.842795591505747
2025-04-11 06:33:59,544 - ***** Epoch: 25: Eval results *****
2025-04-11 06:33:59,545 -   train_loss = 3.7552279593452575
2025-04-11 06:35:40,501 - ***** Epoch: 26: Eval results *****
2025-04-11 06:35:40,502 -   train_loss = 3.690186803303068
2025-04-11 06:37:15,583 - ***** Epoch: 27: Eval results *****
2025-04-11 06:37:15,584 -   train_loss = 3.614418037354
2025-04-11 06:39:00,474 - ***** Epoch: 28: Eval results *****
2025-04-11 06:39:00,475 -   train_loss = 3.5646221826946927
2025-04-11 06:40:31,234 - ***** Epoch: 29: Eval results *****
2025-04-11 06:40:31,234 -   train_loss = 3.502474228541056
2025-04-11 06:42:01,163 - ***** Epoch: 30: Eval results *****
2025-04-11 06:42:01,164 -   train_loss = 3.455972016803802
2025-04-11 06:43:30,626 - ***** Epoch: 31: Eval results *****
2025-04-11 06:43:30,626 -   train_loss = 3.4156347418588306
2025-04-11 06:45:00,503 - ***** Epoch: 32: Eval results *****
2025-04-11 06:45:00,504 -   train_loss = 3.366624105544317
2025-04-11 06:46:30,877 - ***** Epoch: 33: Eval results *****
2025-04-11 06:46:30,877 -   train_loss = 3.332638112325517
2025-04-11 06:48:01,584 - ***** Epoch: 34: Eval results *****
2025-04-11 06:48:01,584 -   train_loss = 3.2871801701803056
2025-04-11 06:49:31,658 - ***** Epoch: 35: Eval results *****
2025-04-11 06:49:31,658 -   train_loss = 3.253458306902931
2025-04-11 06:51:01,529 - ***** Epoch: 36: Eval results *****
2025-04-11 06:51:01,530 -   train_loss = 3.2140008078681097
2025-04-11 06:52:31,330 - ***** Epoch: 37: Eval results *****
2025-04-11 06:52:31,330 -   train_loss = 3.187008959906442
2025-04-11 06:54:01,366 - ***** Epoch: 38: Eval results *****
2025-04-11 06:54:01,366 -   train_loss = 3.1527829435136585
2025-04-11 06:55:31,392 - ***** Epoch: 39: Eval results *****
2025-04-11 06:55:31,392 -   train_loss = 3.119330841397482
2025-04-11 06:57:01,264 - ***** Epoch: 40: Eval results *****
2025-04-11 06:57:01,265 -   train_loss = 3.093865186449081
2025-04-11 06:58:31,494 - ***** Epoch: 41: Eval results *****
2025-04-11 06:58:31,494 -   train_loss = 3.068982722267272
2025-04-11 07:00:02,362 - ***** Epoch: 42: Eval results *****
2025-04-11 07:00:02,363 -   train_loss = 3.0450363613310314
2025-04-11 07:01:33,499 - ***** Epoch: 43: Eval results *****
2025-04-11 07:01:33,500 -   train_loss = 3.0193981253911577
2025-04-11 07:03:04,177 - ***** Epoch: 44: Eval results *****
2025-04-11 07:03:04,177 -   train_loss = 3.0044356414249966
2025-04-11 07:04:35,204 - ***** Epoch: 45: Eval results *****
2025-04-11 07:04:35,204 -   train_loss = 2.9870643237280468
2025-04-11 07:06:06,458 - ***** Epoch: 46: Eval results *****
2025-04-11 07:06:06,458 -   train_loss = 2.9627501396905807
2025-04-11 07:07:38,140 - ***** Epoch: 47: Eval results *****
2025-04-11 07:07:38,140 -   train_loss = 2.9458063670567105
2025-04-11 07:09:09,173 - ***** Epoch: 48: Eval results *****
2025-04-11 07:09:09,173 -   train_loss = 2.925938575986832
2025-04-11 07:10:40,924 - ***** Epoch: 49: Eval results *****
2025-04-11 07:10:40,924 -   train_loss = 2.9171004560258655
2025-04-11 07:12:11,597 - ***** Epoch: 50: Eval results *****
2025-04-11 07:12:11,598 -   train_loss = 2.893362571322729
2025-04-11 07:13:42,171 - ***** Epoch: 51: Eval results *****
2025-04-11 07:13:42,171 -   train_loss = 2.87998774695018
2025-04-11 07:15:12,584 - ***** Epoch: 52: Eval results *****
2025-04-11 07:15:12,585 -   train_loss = 2.859895172573271
2025-04-11 07:16:42,881 - ***** Epoch: 53: Eval results *****
2025-04-11 07:16:42,882 -   train_loss = 2.8519830741579573
2025-04-11 07:18:13,326 - ***** Epoch: 54: Eval results *****
2025-04-11 07:18:13,326 -   train_loss = 2.83311474890936
2025-04-11 07:19:44,026 - ***** Epoch: 55: Eval results *****
2025-04-11 07:19:44,026 -   train_loss = 2.8224828129722956
2025-04-11 07:21:14,403 - ***** Epoch: 56: Eval results *****
2025-04-11 07:21:14,404 -   train_loss = 2.809081092713371
2025-04-11 07:22:45,218 - ***** Epoch: 57: Eval results *****
2025-04-11 07:22:45,218 -   train_loss = 2.800690597958035
2025-04-11 07:24:15,009 - ***** Epoch: 58: Eval results *****
2025-04-11 07:24:15,009 -   train_loss = 2.790201565575978
2025-04-11 07:25:45,646 - ***** Epoch: 59: Eval results *****
2025-04-11 07:25:45,646 -   train_loss = 2.772611107145037
2025-04-11 07:27:16,210 - ***** Epoch: 60: Eval results *****
2025-04-11 07:27:16,210 -   train_loss = 2.771562110810053
2025-04-11 07:28:45,938 - ***** Epoch: 61: Eval results *****
2025-04-11 07:28:45,938 -   train_loss = 2.7584266284155468
2025-04-11 07:30:15,925 - ***** Epoch: 62: Eval results *****
2025-04-11 07:30:15,925 -   train_loss = 2.749404714221046
2025-04-11 07:31:46,045 - ***** Epoch: 63: Eval results *****
2025-04-11 07:31:46,046 -   train_loss = 2.7378107101198226
2025-04-11 07:33:15,899 - ***** Epoch: 64: Eval results *****
2025-04-11 07:33:15,899 -   train_loss = 2.731044477886624
2025-04-11 07:34:45,854 - ***** Epoch: 65: Eval results *****
2025-04-11 07:34:45,854 -   train_loss = 2.718774670646304
2025-04-11 07:36:15,396 - ***** Epoch: 66: Eval results *****
2025-04-11 07:36:15,397 -   train_loss = 2.712716095031254
2025-04-11 07:37:45,219 - ***** Epoch: 67: Eval results *****
2025-04-11 07:37:45,219 -   train_loss = 2.712129555051289
2025-04-11 07:39:15,461 - ***** Epoch: 68: Eval results *****
2025-04-11 07:39:15,461 -   train_loss = 2.7062945630815296
2025-04-11 07:40:46,026 - ***** Epoch: 69: Eval results *****
2025-04-11 07:40:46,027 -   train_loss = 2.700396147985307
2025-04-11 07:42:16,246 - ***** Epoch: 70: Eval results *****
2025-04-11 07:42:16,247 -   train_loss = 2.690618183877733
2025-04-11 07:43:46,904 - ***** Epoch: 71: Eval results *****
2025-04-11 07:43:46,904 -   train_loss = 2.682138927399166
2025-04-11 07:45:17,902 - ***** Epoch: 72: Eval results *****
2025-04-11 07:45:17,903 -   train_loss = 2.6827414111485557
2025-04-11 07:46:48,984 - ***** Epoch: 73: Eval results *****
2025-04-11 07:46:48,984 -   train_loss = 2.6808212976607066
2025-04-11 07:48:19,525 - ***** Epoch: 74: Eval results *****
2025-04-11 07:48:19,526 -   train_loss = 2.671790035944136
2025-04-11 07:49:49,956 - ***** Epoch: 75: Eval results *****
2025-04-11 07:49:49,956 -   train_loss = 2.6696815604255315
2025-04-11 07:51:20,907 - ***** Epoch: 76: Eval results *****
2025-04-11 07:51:20,908 -   train_loss = 2.6595336823236373
2025-04-11 07:52:50,985 - ***** Epoch: 77: Eval results *****
2025-04-11 07:52:50,985 -   train_loss = 2.6572705772187977
2025-04-11 07:54:22,504 - ***** Epoch: 78: Eval results *****
2025-04-11 07:54:22,505 -   train_loss = 2.6542125694335454
2025-04-11 07:55:54,319 - ***** Epoch: 79: Eval results *****
2025-04-11 07:55:54,320 -   train_loss = 2.6537782691773915
2025-04-11 07:57:27,349 - ***** Epoch: 80: Eval results *****
2025-04-11 07:57:27,349 -   train_loss = 2.6492542028427124
2025-04-11 07:59:09,213 - ***** Epoch: 81: Eval results *****
2025-04-11 07:59:09,213 -   train_loss = 2.6375844194775535
2025-04-11 08:00:49,610 - ***** Epoch: 82: Eval results *****
2025-04-11 08:00:49,610 -   train_loss = 2.634187336951967
2025-04-11 08:02:27,126 - ***** Epoch: 83: Eval results *****
2025-04-11 08:02:27,126 -   train_loss = 2.638562034046839
2025-04-11 08:03:57,201 - ***** Epoch: 84: Eval results *****
2025-04-11 08:03:57,201 -   train_loss = 2.6337586092570473
2025-04-11 08:05:25,497 - ***** Epoch: 85: Eval results *****
2025-04-11 08:05:25,498 -   train_loss = 2.6359572864714123
2025-04-11 08:06:52,957 - ***** Epoch: 86: Eval results *****
2025-04-11 08:06:52,958 -   train_loss = 2.6322764744834295
2025-04-11 08:08:20,998 - ***** Epoch: 87: Eval results *****
2025-04-11 08:08:20,999 -   train_loss = 2.6246725055906506
2025-04-11 08:09:48,582 - ***** Epoch: 88: Eval results *****
2025-04-11 08:09:48,583 -   train_loss = 2.6316468772434054
2025-04-11 08:11:15,961 - ***** Epoch: 89: Eval results *****
2025-04-11 08:11:15,961 -   train_loss = 2.626974976251996
2025-04-11 08:12:45,186 - ***** Epoch: 90: Eval results *****
2025-04-11 08:12:45,186 -   train_loss = 2.6211718699288746
2025-04-11 08:14:14,082 - ***** Epoch: 91: Eval results *****
2025-04-11 08:14:14,083 -   train_loss = 2.61761236380017
2025-04-11 08:15:42,970 - ***** Epoch: 92: Eval results *****
2025-04-11 08:15:42,970 -   train_loss = 2.620396025597103
2025-04-11 08:17:11,877 - ***** Epoch: 93: Eval results *****
2025-04-11 08:17:11,877 -   train_loss = 2.6216018805428156
2025-04-11 08:18:40,606 - ***** Epoch: 94: Eval results *****
2025-04-11 08:18:40,606 -   train_loss = 2.62243622257596
2025-04-11 08:20:09,426 - ***** Epoch: 95: Eval results *****
2025-04-11 08:20:09,427 -   train_loss = 2.6182527504270037
2025-04-11 08:21:38,224 - ***** Epoch: 96: Eval results *****
2025-04-11 08:21:38,224 -   train_loss = 2.620199137263828
2025-04-11 08:23:06,077 - ***** Epoch: 97: Eval results *****
2025-04-11 08:23:06,077 -   train_loss = 2.616353157966856
2025-04-11 08:24:32,559 - ***** Epoch: 98: Eval results *****
2025-04-11 08:24:32,559 -   train_loss = 2.620812938326881
2025-04-11 08:26:00,613 - ***** Epoch: 99: Eval results *****
2025-04-11 08:26:00,613 -   train_loss = 2.615615714164007
2025-04-11 08:27:28,547 - ***** Epoch: 100: Eval results *****
2025-04-11 08:27:28,548 -   train_loss = 2.6214263174268932
2025-04-11 08:27:29,088 - Pre-training finished...
2025-04-11 08:27:29,646 - Freeze all parameters but the last layer for efficiency
2025-04-11 08:27:29,657 - Multimodal Intent Recognition begins...
2025-04-11 08:27:29,657 - Training begins...
2025-04-11 08:28:17,672 - Initializing centroids with K-means++...
2025-04-11 08:28:17,947 - K-means++ used 0.27 s
2025-04-11 08:30:33,530 - K-means used 0.17 s
2025-04-11 08:30:37,096 - ***** Epoch: 1 *****
2025-04-11 08:30:37,096 - Supervised Training Loss: 5.687280
2025-04-11 08:30:37,097 - Unsupervised Training Loss: 5.879240
2025-04-11 08:32:52,684 - K-means used 0.06 s
2025-04-11 08:32:56,853 - ***** Epoch: 2 *****
2025-04-11 08:32:56,854 - Supervised Training Loss: 5.800750
2025-04-11 08:32:56,854 - Unsupervised Training Loss: 5.862310
2025-04-11 08:35:16,904 - K-means used 0.13 s
2025-04-11 08:35:21,876 - ***** Epoch: 3 *****
2025-04-11 08:35:21,876 - Supervised Training Loss: 5.846620
2025-04-11 08:35:21,876 - Unsupervised Training Loss: 5.902710
2025-04-11 08:37:37,996 - K-means used 0.17 s
2025-04-11 08:37:43,668 - ***** Epoch: 4 *****
2025-04-11 08:37:43,669 - Supervised Training Loss: 5.866710
2025-04-11 08:37:43,669 - Unsupervised Training Loss: 5.898580
2025-04-11 08:39:58,655 - K-means used 0.06 s
2025-04-11 08:40:05,268 - ***** Epoch: 5 *****
2025-04-11 08:40:05,268 - Supervised Training Loss: 5.882210
2025-04-11 08:40:05,269 - Unsupervised Training Loss: 5.893640
2025-04-11 08:42:21,505 - K-means used 0.09 s
2025-04-11 08:42:29,924 - ***** Epoch: 6 *****
2025-04-11 08:42:29,924 - Supervised Training Loss: 5.889770
2025-04-11 08:42:29,924 - Unsupervised Training Loss: 5.889250
2025-04-11 08:44:55,518 - K-means used 0.17 s
2025-04-11 08:45:05,502 - ***** Epoch: 7 *****
2025-04-11 08:45:05,503 - Supervised Training Loss: 5.897590
2025-04-11 08:45:05,503 - Unsupervised Training Loss: 5.880980
2025-04-11 08:47:31,709 - K-means used 0.05 s
2025-04-11 08:47:44,866 - ***** Epoch: 8 *****
2025-04-11 08:47:44,866 - Supervised Training Loss: 5.798370
2025-04-11 08:47:44,866 - Unsupervised Training Loss: 5.871130
2025-04-11 08:50:19,600 - K-means used 0.11 s
2025-04-11 08:50:40,006 - ***** Epoch: 9 *****
2025-04-11 08:50:40,007 - Supervised Training Loss: 5.848380
2025-04-11 08:50:40,007 - Unsupervised Training Loss: 5.853100
2025-04-11 08:53:06,060 - K-means used 0.07 s
2025-04-11 08:53:27,884 - ***** Epoch: 10 *****
2025-04-11 08:53:27,884 - Supervised Training Loss: 5.862320
2025-04-11 08:53:27,884 - Unsupervised Training Loss: 5.833300
2025-04-11 08:56:25,972 - K-means used 0.07 s
2025-04-11 08:56:47,943 - ***** Epoch: 11 *****
2025-04-11 08:56:47,944 - Supervised Training Loss: 5.875690
2025-04-11 08:56:47,944 - Unsupervised Training Loss: 5.734510
2025-04-11 08:59:31,367 - K-means used 0.07 s
2025-04-11 08:59:52,564 - ***** Epoch: 12 *****
2025-04-11 08:59:52,564 - Supervised Training Loss: 5.883810
2025-04-11 08:59:52,565 - Unsupervised Training Loss: 5.894090
2025-04-11 09:02:06,628 - K-means used 0.12 s
2025-04-11 09:02:30,331 - ***** Epoch: 13 *****
2025-04-11 09:02:30,331 - Supervised Training Loss: 5.889350
2025-04-11 09:02:30,331 - Unsupervised Training Loss: 5.885980
2025-04-11 09:04:43,952 - K-means used 0.05 s
2025-04-11 09:05:09,605 - ***** Epoch: 14 *****
2025-04-11 09:05:09,605 - Supervised Training Loss: 5.893600
2025-04-11 09:05:09,605 - Unsupervised Training Loss: 5.873150
2025-04-11 09:07:24,734 - K-means used 0.05 s
2025-04-11 09:07:52,402 - ***** Epoch: 15 *****
2025-04-11 09:07:52,402 - Supervised Training Loss: 5.897460
2025-04-11 09:07:52,402 - Unsupervised Training Loss: 5.847100
2025-04-11 09:10:05,738 - K-means used 0.05 s
2025-04-11 09:10:33,562 - ***** Epoch: 16 *****
2025-04-11 09:10:33,562 - Supervised Training Loss: 5.802860
2025-04-11 09:10:33,562 - Unsupervised Training Loss: 5.810590
2025-04-11 09:12:46,189 - K-means used 0.04 s
2025-04-11 09:13:15,825 - ***** Epoch: 17 *****
2025-04-11 09:13:15,825 - Supervised Training Loss: 5.864530
2025-04-11 09:13:15,826 - Unsupervised Training Loss: 5.717090
2025-04-11 09:14:47,173 - Training is finished...
2025-04-11 09:14:47,173 - Testing begins...
2025-04-11 09:15:20,502 - ***** Test results *****
2025-04-11 09:15:20,502 -   ACC = 38.49
2025-04-11 09:15:20,502 -   ARI = 24.55
2025-04-11 09:15:20,502 -   NMI = 23.34
2025-04-11 09:15:20,502 -   fmi = 38.24
2025-04-11 09:15:20,502 - Testing is finished...
2025-04-11 09:15:20,502 - Multimodal intent recognition is finished...
2025-04-11 09:15:20,502 - Results are saved in results/results_umc.csv
