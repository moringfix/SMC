2025-04-14 15:57:51,032 - ============================== Params ==============================
2025-04-14 15:57:51,033 - logger_name: umc_umc_bert-base-uncased_MELD-DA_0
2025-04-14 15:57:51,033 - dataset: MELD-DA
2025-04-14 15:57:51,033 - multimodal_method: umc
2025-04-14 15:57:51,033 - method: umc
2025-04-14 15:57:51,033 - text_backbone: bert-base-uncased
2025-04-14 15:57:51,033 - seed: 0
2025-04-14 15:57:51,033 - num_workers: 16
2025-04-14 15:57:51,033 - log_id: umc_umc_bert-base-uncased_MELD-DA_0_2025-04-14-15-57-51
2025-04-14 15:57:51,033 - gpu_id: 1
2025-04-14 15:57:51,033 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-14 15:57:51,033 - train: True
2025-04-14 15:57:51,033 - tune: True
2025-04-14 15:57:51,033 - save_model: True
2025-04-14 15:57:51,033 - save_results: True
2025-04-14 15:57:51,033 - log_path: logs
2025-04-14 15:57:51,033 - cache_path: cache
2025-04-14 15:57:51,033 - video_data_path: video_data
2025-04-14 15:57:51,033 - audio_data_path: audio_data
2025-04-14 15:57:51,033 - video_feats_path: swin_feats.pkl
2025-04-14 15:57:51,033 - audio_feats_path: wavlm_feats.pkl
2025-04-14 15:57:51,033 - results_path: results
2025-04-14 15:57:51,033 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA
2025-04-14 15:57:51,033 - model_path: models
2025-04-14 15:57:51,034 - config_file_name: umc_MELD-DA
2025-04-14 15:57:51,034 - results_file_name: results_umc.csv
2025-04-14 15:57:51,034 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-14 15:57:51,034 - text_seq_len: 70
2025-04-14 15:57:51,034 - video_seq_len: 250
2025-04-14 15:57:51,034 - audio_seq_len: 520
2025-04-14 15:57:51,034 - text_feat_dim: 768
2025-04-14 15:57:51,034 - video_feat_dim: 1024
2025-04-14 15:57:51,034 - audio_feat_dim: 768
2025-04-14 15:57:51,034 - num_labels: 12
2025-04-14 15:57:51,034 - num_train_examples: 7990
2025-04-14 15:57:51,034 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-14 15:57:51,034 - pretrain_batch_size: 128
2025-04-14 15:57:51,034 - train_batch_size: 128
2025-04-14 15:57:51,034 - eval_batch_size: 128
2025-04-14 15:57:51,034 - test_batch_size: 128
2025-04-14 15:57:51,034 - num_pretrain_epochs: [100]
2025-04-14 15:57:51,034 - num_train_epochs: [100]
2025-04-14 15:57:51,034 - pretrain: [True]
2025-04-14 15:57:51,034 - aligned_method: ctc
2025-04-14 15:57:51,034 - need_aligned: False
2025-04-14 15:57:51,034 - freeze_pretrain_bert_parameters: True
2025-04-14 15:57:51,034 - freeze_train_bert_parameters: True
2025-04-14 15:57:51,034 - pretrain_temperature: [0.2]
2025-04-14 15:57:51,034 - train_temperature_sup: [4]
2025-04-14 15:57:51,034 - train_temperature_unsup: [4]
2025-04-14 15:57:51,034 - activation: tanh
2025-04-14 15:57:51,035 - lr_pre: [5e-06]
2025-04-14 15:57:51,035 - lr: [0.0002]
2025-04-14 15:57:51,035 - delta: [0.05]
2025-04-14 15:57:51,035 - thres: [0.1]
2025-04-14 15:57:51,035 - topk: [5]
2025-04-14 15:57:51,035 - weight_decay: 0.01
2025-04-14 15:57:51,035 - feat_dim: 768
2025-04-14 15:57:51,035 - hidden_size: 768
2025-04-14 15:57:51,035 - grad_clip: [-1.0]
2025-04-14 15:57:51,035 - warmup_proportion: 0.1
2025-04-14 15:57:51,035 - hidden_dropout_prob: 0.1
2025-04-14 15:57:51,035 - weight: 1.0
2025-04-14 15:57:51,035 - loss_mode: rdrop
2025-04-14 15:57:51,035 - base_dim: [256]
2025-04-14 15:57:51,035 - nheads: 8
2025-04-14 15:57:51,035 - attn_dropout: 0.1
2025-04-14 15:57:51,035 - relu_dropout: 0.1
2025-04-14 15:57:51,035 - embed_dropout: 0.01
2025-04-14 15:57:51,036 - res_dropout: 0.1
2025-04-14 15:57:51,036 - attn_mask: True
2025-04-14 15:57:51,036 - encoder_layers_1: 1
2025-04-14 15:57:51,036 - fusion_act: tanh
2025-04-14 15:57:51,036 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0
2025-04-14 15:57:51,036 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0/models
2025-04-14 15:57:51,036 - ============================== End Params ==============================
2025-04-14 15:57:52,209 - Freeze all parameters but the last layer for efficiency
2025-04-14 15:57:52,247 - Pre-training start...
2025-04-14 15:59:35,222 - ***** Epoch: 1: Eval results *****
2025-04-14 15:59:35,222 -   train_loss = 5.908864475431896
2025-04-14 16:01:15,792 - ***** Epoch: 2: Eval results *****
2025-04-14 16:01:15,792 -   train_loss = 5.901010475461445
2025-04-14 16:02:56,179 - ***** Epoch: 3: Eval results *****
2025-04-14 16:02:56,179 -   train_loss = 5.8903142384120395
2025-04-14 16:04:36,409 - ***** Epoch: 4: Eval results *****
2025-04-14 16:04:36,410 -   train_loss = 5.852486186557346
2025-04-14 16:06:15,707 - ***** Epoch: 5: Eval results *****
2025-04-14 16:06:15,707 -   train_loss = 5.753782741607181
2025-04-14 16:07:54,828 - ***** Epoch: 6: Eval results *****
2025-04-14 16:07:54,828 -   train_loss = 5.498170913211883
2025-04-14 16:09:33,693 - ***** Epoch: 7: Eval results *****
2025-04-14 16:09:33,693 -   train_loss = 5.111781846909296
2025-04-14 16:11:12,623 - ***** Epoch: 8: Eval results *****
2025-04-14 16:11:12,624 -   train_loss = 4.665356060815236
2025-04-14 16:12:52,392 - ***** Epoch: 9: Eval results *****
2025-04-14 16:12:52,393 -   train_loss = 4.30143201918829
2025-04-14 16:14:31,496 - ***** Epoch: 10: Eval results *****
2025-04-14 16:14:31,496 -   train_loss = 4.018469288235619
2025-04-14 16:16:10,764 - ***** Epoch: 11: Eval results *****
2025-04-14 16:16:10,764 -   train_loss = 3.8066567731282066
2025-04-14 16:17:49,652 - ***** Epoch: 12: Eval results *****
2025-04-14 16:17:49,653 -   train_loss = 3.6293212118602933
