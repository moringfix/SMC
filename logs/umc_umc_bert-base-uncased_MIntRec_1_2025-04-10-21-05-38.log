2025-04-10 21:05:38,935 - ============================== Params ==============================
2025-04-10 21:05:38,935 - logger_name: umc_umc_bert-base-uncased_MIntRec_1
2025-04-10 21:05:38,935 - dataset: MIntRec
2025-04-10 21:05:38,935 - multimodal_method: umc
2025-04-10 21:05:38,935 - method: umc
2025-04-10 21:05:38,935 - text_backbone: bert-base-uncased
2025-04-10 21:05:38,936 - seed: 1
2025-04-10 21:05:38,936 - num_workers: 16
2025-04-10 21:05:38,936 - log_id: umc_umc_bert-base-uncased_MIntRec_1_2025-04-10-21-05-38
2025-04-10 21:05:38,936 - gpu_id: 0
2025-04-10 21:05:38,936 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-10 21:05:38,936 - train: True
2025-04-10 21:05:38,936 - tune: True
2025-04-10 21:05:38,936 - save_model: True
2025-04-10 21:05:38,936 - save_results: True
2025-04-10 21:05:38,936 - log_path: logs
2025-04-10 21:05:38,936 - cache_path: cache
2025-04-10 21:05:38,936 - video_data_path: video_data
2025-04-10 21:05:38,936 - audio_data_path: audio_data
2025-04-10 21:05:38,936 - video_feats_path: swin_feats.pkl
2025-04-10 21:05:38,936 - audio_feats_path: wavlm_feats.pkl
2025-04-10 21:05:38,936 - results_path: results
2025-04-10 21:05:38,936 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec
2025-04-10 21:05:38,936 - model_path: models
2025-04-10 21:05:38,936 - config_file_name: umc_MIntRec
2025-04-10 21:05:38,936 - results_file_name: results_umc.csv
2025-04-10 21:05:38,936 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-10 21:05:38,936 - text_seq_len: 30
2025-04-10 21:05:38,936 - video_seq_len: 230
2025-04-10 21:05:38,936 - audio_seq_len: 480
2025-04-10 21:05:38,936 - text_feat_dim: 768
2025-04-10 21:05:38,936 - video_feat_dim: 1024
2025-04-10 21:05:38,936 - audio_feat_dim: 768
2025-04-10 21:05:38,936 - num_labels: 20
2025-04-10 21:05:38,937 - num_train_examples: 1779
2025-04-10 21:05:38,937 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-10 21:05:38,937 - pretrain_batch_size: 128
2025-04-10 21:05:38,937 - train_batch_size: 128
2025-04-10 21:05:38,937 - eval_batch_size: 128
2025-04-10 21:05:38,937 - test_batch_size: 128
2025-04-10 21:05:38,937 - num_pretrain_epochs: 100
2025-04-10 21:05:38,937 - num_train_epochs: 100
2025-04-10 21:05:38,937 - pretrain: [True]
2025-04-10 21:05:38,937 - aligned_method: ctc
2025-04-10 21:05:38,937 - need_aligned: False
2025-04-10 21:05:38,937 - freeze_pretrain_bert_parameters: [True]
2025-04-10 21:05:38,937 - freeze_train_bert_parameters: [True]
2025-04-10 21:05:38,937 - pretrain_temperature: [0.2]
2025-04-10 21:05:38,937 - train_temperature_sup: [1.4]
2025-04-10 21:05:38,937 - train_temperature_unsup: [1]
2025-04-10 21:05:38,937 - activation: tanh
2025-04-10 21:05:38,937 - lr_pre: 2e-05
2025-04-10 21:05:38,937 - lr: [0.0003]
2025-04-10 21:05:38,937 - delta: [0.05]
2025-04-10 21:05:38,937 - thres: [0.1]
2025-04-10 21:05:38,937 - topk: [5]
2025-04-10 21:05:38,937 - weight_decay: 0.01
2025-04-10 21:05:38,937 - feat_dim: 768
2025-04-10 21:05:38,937 - hidden_size: 768
2025-04-10 21:05:38,937 - grad_clip: -1.0
2025-04-10 21:05:38,937 - warmup_proportion: 0.1
2025-04-10 21:05:38,937 - hidden_dropout_prob: 0.1
2025-04-10 21:05:38,937 - weight: 1.0
2025-04-10 21:05:38,937 - loss_mode: rdrop
2025-04-10 21:05:38,938 - base_dim: 256
2025-04-10 21:05:38,938 - nheads: 8
2025-04-10 21:05:38,938 - attn_dropout: 0.1
2025-04-10 21:05:38,938 - relu_dropout: 0.1
2025-04-10 21:05:38,938 - embed_dropout: 0.1
2025-04-10 21:05:38,938 - res_dropout: 0.0
2025-04-10 21:05:38,938 - attn_mask: True
2025-04-10 21:05:38,938 - encoder_layers_1: 1
2025-04-10 21:05:38,938 - fusion_act: tanh
2025-04-10 21:05:38,938 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec/umc_umc_MIntRec_bert-base-uncased_1
2025-04-10 21:05:38,938 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec/umc_umc_MIntRec_bert-base-uncased_1/models
2025-04-10 21:05:38,938 - ============================== End Params ==============================
2025-04-10 21:05:40,010 - Freeze all parameters but the last layer for efficiency
2025-04-10 21:05:40,045 - Pre-training start...
2025-04-10 21:05:53,793 - ***** Epoch: 1: Eval results *****
2025-04-10 21:05:53,793 -   train_loss = 5.9414922169276645
2025-04-10 21:06:06,995 - ***** Epoch: 2: Eval results *****
2025-04-10 21:06:06,995 -   train_loss = 5.941111019679478
2025-04-10 21:06:20,300 - ***** Epoch: 3: Eval results *****
2025-04-10 21:06:20,300 -   train_loss = 5.934085130691528
2025-04-10 21:06:36,066 - ***** Epoch: 4: Eval results *****
2025-04-10 21:06:36,066 -   train_loss = 5.929164954594204
2025-04-10 21:06:52,233 - ***** Epoch: 5: Eval results *****
2025-04-10 21:06:52,233 -   train_loss = 5.921181985310146
2025-04-10 21:07:08,122 - ***** Epoch: 6: Eval results *****
2025-04-10 21:07:08,122 -   train_loss = 5.892749037061419
2025-04-10 21:07:23,619 - ***** Epoch: 7: Eval results *****
2025-04-10 21:07:23,620 -   train_loss = 5.823347840990339
2025-04-10 21:07:39,244 - ***** Epoch: 8: Eval results *****
2025-04-10 21:07:39,244 -   train_loss = 5.5911979334695
2025-04-10 21:07:55,128 - ***** Epoch: 9: Eval results *****
2025-04-10 21:07:55,128 -   train_loss = 5.148698636463711
2025-04-10 21:08:10,753 - ***** Epoch: 10: Eval results *****
2025-04-10 21:08:10,753 -   train_loss = 4.642203978129795
2025-04-10 21:08:25,740 - ***** Epoch: 11: Eval results *****
2025-04-10 21:08:25,740 -   train_loss = 4.220354199409485
2025-04-10 21:08:40,349 - ***** Epoch: 12: Eval results *****
2025-04-10 21:08:40,349 -   train_loss = 3.9630529369626726
2025-04-10 21:08:54,804 - ***** Epoch: 13: Eval results *****
2025-04-10 21:08:54,805 -   train_loss = 3.7664303609303067
2025-04-10 21:09:08,960 - ***** Epoch: 14: Eval results *****
2025-04-10 21:09:08,960 -   train_loss = 3.610994270869664
2025-04-10 21:09:23,396 - ***** Epoch: 15: Eval results *****
2025-04-10 21:09:23,396 -   train_loss = 3.4929807697023665
2025-04-10 21:09:37,734 - ***** Epoch: 16: Eval results *****
2025-04-10 21:09:37,734 -   train_loss = 3.3967948811394826
2025-04-10 21:09:52,104 - ***** Epoch: 17: Eval results *****
2025-04-10 21:09:52,104 -   train_loss = 3.330802287374224
2025-04-10 21:10:06,813 - ***** Epoch: 18: Eval results *****
2025-04-10 21:10:06,814 -   train_loss = 3.265219569206238
