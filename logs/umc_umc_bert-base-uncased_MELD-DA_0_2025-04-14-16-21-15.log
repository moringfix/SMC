2025-04-14 16:21:15,753 - ============================== Params ==============================
2025-04-14 16:21:15,753 - logger_name: umc_umc_bert-base-uncased_MELD-DA_0
2025-04-14 16:21:15,753 - dataset: MELD-DA
2025-04-14 16:21:15,754 - multimodal_method: umc
2025-04-14 16:21:15,754 - method: umc
2025-04-14 16:21:15,754 - text_backbone: bert-base-uncased
2025-04-14 16:21:15,754 - seed: 0
2025-04-14 16:21:15,754 - num_workers: 16
2025-04-14 16:21:15,754 - log_id: umc_umc_bert-base-uncased_MELD-DA_0_2025-04-14-16-21-15
2025-04-14 16:21:15,754 - gpu_id: 1
2025-04-14 16:21:15,754 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-14 16:21:15,754 - train: True
2025-04-14 16:21:15,754 - tune: True
2025-04-14 16:21:15,754 - save_model: True
2025-04-14 16:21:15,754 - save_results: True
2025-04-14 16:21:15,754 - log_path: logs
2025-04-14 16:21:15,754 - cache_path: cache
2025-04-14 16:21:15,754 - video_data_path: video_data
2025-04-14 16:21:15,754 - audio_data_path: audio_data
2025-04-14 16:21:15,754 - video_feats_path: swin_feats.pkl
2025-04-14 16:21:15,754 - audio_feats_path: wavlm_feats.pkl
2025-04-14 16:21:15,754 - results_path: results
2025-04-14 16:21:15,754 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA
2025-04-14 16:21:15,754 - model_path: models
2025-04-14 16:21:15,754 - config_file_name: umc_MELD-DA
2025-04-14 16:21:15,754 - results_file_name: results_umc.csv
2025-04-14 16:21:15,754 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-14 16:21:15,754 - text_seq_len: 70
2025-04-14 16:21:15,754 - video_seq_len: 250
2025-04-14 16:21:15,754 - audio_seq_len: 520
2025-04-14 16:21:15,754 - text_feat_dim: 768
2025-04-14 16:21:15,755 - video_feat_dim: 1024
2025-04-14 16:21:15,755 - audio_feat_dim: 768
2025-04-14 16:21:15,755 - num_labels: 12
2025-04-14 16:21:15,755 - num_train_examples: 7990
2025-04-14 16:21:15,755 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-14 16:21:15,755 - pretrain_batch_size: 128
2025-04-14 16:21:15,755 - train_batch_size: 128
2025-04-14 16:21:15,755 - eval_batch_size: 128
2025-04-14 16:21:15,755 - test_batch_size: 128
2025-04-14 16:21:15,755 - num_pretrain_epochs: [100]
2025-04-14 16:21:15,755 - num_train_epochs: [100]
2025-04-14 16:21:15,755 - pretrain: [True]
2025-04-14 16:21:15,755 - aligned_method: ctc
2025-04-14 16:21:15,755 - need_aligned: False
2025-04-14 16:21:15,755 - freeze_pretrain_bert_parameters: True
2025-04-14 16:21:15,755 - freeze_train_bert_parameters: True
2025-04-14 16:21:15,755 - pretrain_temperature: [0.2]
2025-04-14 16:21:15,756 - train_temperature_sup: [4]
2025-04-14 16:21:15,756 - train_temperature_unsup: [4]
2025-04-14 16:21:15,756 - activation: tanh
2025-04-14 16:21:15,756 - lr_pre: [5e-06]
2025-04-14 16:21:15,756 - lr: [0.0002]
2025-04-14 16:21:15,756 - delta: [0.05]
2025-04-14 16:21:15,756 - thres: [0.1]
2025-04-14 16:21:15,756 - topk: [5]
2025-04-14 16:21:15,756 - weight_decay: 0.01
2025-04-14 16:21:15,756 - feat_dim: 768
2025-04-14 16:21:15,756 - hidden_size: 768
2025-04-14 16:21:15,756 - grad_clip: [-1.0]
2025-04-14 16:21:15,756 - warmup_proportion: 0.1
2025-04-14 16:21:15,756 - hidden_dropout_prob: 0.1
2025-04-14 16:21:15,756 - weight: 1.0
2025-04-14 16:21:15,756 - loss_mode: rdrop
2025-04-14 16:21:15,756 - base_dim: [256]
2025-04-14 16:21:15,756 - nheads: 8
2025-04-14 16:21:15,756 - attn_dropout: 0.1
2025-04-14 16:21:15,756 - relu_dropout: 0.1
2025-04-14 16:21:15,756 - embed_dropout: 0.01
2025-04-14 16:21:15,756 - res_dropout: 0.1
2025-04-14 16:21:15,756 - attn_mask: True
2025-04-14 16:21:15,756 - encoder_layers_1: 1
2025-04-14 16:21:15,756 - fusion_act: tanh
2025-04-14 16:21:15,756 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0
2025-04-14 16:21:15,756 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0/models
2025-04-14 16:21:15,756 - ============================== End Params ==============================
2025-04-14 16:21:17,461 - Freeze all parameters but the last layer for efficiency
2025-04-14 16:21:17,500 - Pre-training start...
2025-04-14 16:22:52,099 - ***** Epoch: 1: Eval results *****
2025-04-14 16:22:52,099 -   train_loss = 5.908864475431896
2025-04-14 16:24:31,360 - ***** Epoch: 2: Eval results *****
2025-04-14 16:24:31,361 -   train_loss = 5.901010475461445
2025-04-14 16:26:11,211 - ***** Epoch: 3: Eval results *****
2025-04-14 16:26:11,212 -   train_loss = 5.8903142384120395
2025-04-14 16:27:50,213 - ***** Epoch: 4: Eval results *****
2025-04-14 16:27:50,214 -   train_loss = 5.852486186557346
2025-04-14 16:29:28,810 - ***** Epoch: 5: Eval results *****
2025-04-14 16:29:28,810 -   train_loss = 5.753782741607181
2025-04-14 16:31:06,653 - ***** Epoch: 6: Eval results *****
2025-04-14 16:31:06,654 -   train_loss = 5.498170913211883
2025-04-14 16:32:46,212 - ***** Epoch: 7: Eval results *****
2025-04-14 16:32:46,212 -   train_loss = 5.111781846909296
2025-04-14 16:34:25,864 - ***** Epoch: 8: Eval results *****
2025-04-14 16:34:25,864 -   train_loss = 4.665356060815236
2025-04-14 16:36:05,418 - ***** Epoch: 9: Eval results *****
2025-04-14 16:36:05,418 -   train_loss = 4.30143201918829
2025-04-14 16:37:45,525 - ***** Epoch: 10: Eval results *****
2025-04-14 16:37:45,525 -   train_loss = 4.018469288235619
2025-04-14 16:39:27,880 - ***** Epoch: 11: Eval results *****
2025-04-14 16:39:27,880 -   train_loss = 3.8066567731282066
2025-04-14 16:41:09,906 - ***** Epoch: 12: Eval results *****
2025-04-14 16:41:09,906 -   train_loss = 3.6293212118602933
2025-04-14 16:42:51,306 - ***** Epoch: 13: Eval results *****
2025-04-14 16:42:51,307 -   train_loss = 3.4989640031542097
2025-04-14 16:44:32,611 - ***** Epoch: 14: Eval results *****
2025-04-14 16:44:32,612 -   train_loss = 3.4071338555169484
2025-04-14 16:46:14,945 - ***** Epoch: 15: Eval results *****
2025-04-14 16:46:14,946 -   train_loss = 3.322564953849429
2025-04-14 16:47:57,152 - ***** Epoch: 16: Eval results *****
2025-04-14 16:47:57,152 -   train_loss = 3.259733862347073
2025-04-14 16:49:39,041 - ***** Epoch: 17: Eval results *****
2025-04-14 16:49:39,042 -   train_loss = 3.217532207095434
2025-04-14 16:51:21,207 - ***** Epoch: 18: Eval results *****
2025-04-14 16:51:21,207 -   train_loss = 3.175380824104188
2025-04-14 16:53:01,501 - ***** Epoch: 19: Eval results *****
2025-04-14 16:53:01,501 -   train_loss = 3.138942495224968
2025-04-14 16:54:41,488 - ***** Epoch: 20: Eval results *****
2025-04-14 16:54:41,488 -   train_loss = 3.108795729894487
2025-04-14 16:56:23,820 - ***** Epoch: 21: Eval results *****
2025-04-14 16:56:23,821 -   train_loss = 3.0804221592252214
2025-04-14 16:58:01,783 - ***** Epoch: 22: Eval results *****
2025-04-14 16:58:01,783 -   train_loss = 3.053323885751149
2025-04-14 16:59:42,361 - ***** Epoch: 23: Eval results *****
2025-04-14 16:59:42,361 -   train_loss = 3.031118222645351
2025-04-14 17:01:21,995 - ***** Epoch: 24: Eval results *****
2025-04-14 17:01:21,996 -   train_loss = 3.0055427286359997
2025-04-14 17:03:01,261 - ***** Epoch: 25: Eval results *****
2025-04-14 17:03:01,261 -   train_loss = 2.985558687694489
2025-04-14 17:04:40,338 - ***** Epoch: 26: Eval results *****
2025-04-14 17:04:40,338 -   train_loss = 2.9599734071701294
2025-04-14 17:06:19,133 - ***** Epoch: 27: Eval results *****
2025-04-14 17:06:19,133 -   train_loss = 2.944331785989186
2025-04-14 17:08:06,123 - ***** Epoch: 28: Eval results *****
2025-04-14 17:08:06,124 -   train_loss = 2.9273700903332425
2025-04-14 17:09:54,288 - ***** Epoch: 29: Eval results *****
2025-04-14 17:09:54,289 -   train_loss = 2.9141449360620406
2025-04-14 17:11:37,268 - ***** Epoch: 30: Eval results *****
2025-04-14 17:11:37,268 -   train_loss = 2.892192802731953
2025-04-14 17:13:16,909 - ***** Epoch: 31: Eval results *****
2025-04-14 17:13:16,910 -   train_loss = 2.8829272853003607
2025-04-14 17:14:51,754 - ***** Epoch: 32: Eval results *****
2025-04-14 17:14:51,755 -   train_loss = 2.8661040767790777
2025-04-14 17:16:26,207 - ***** Epoch: 33: Eval results *****
2025-04-14 17:16:26,208 -   train_loss = 2.852936305697002
2025-04-14 17:18:01,182 - ***** Epoch: 34: Eval results *****
2025-04-14 17:18:01,182 -   train_loss = 2.835072880699521
2025-04-14 17:19:35,930 - ***** Epoch: 35: Eval results *****
2025-04-14 17:19:35,931 -   train_loss = 2.8274779433295842
2025-04-14 17:21:14,031 - ***** Epoch: 36: Eval results *****
2025-04-14 17:21:14,031 -   train_loss = 2.815828879674276
2025-04-14 17:22:51,569 - ***** Epoch: 37: Eval results *****
2025-04-14 17:22:51,570 -   train_loss = 2.8105145408993675
2025-04-14 17:24:26,219 - ***** Epoch: 38: Eval results *****
2025-04-14 17:24:26,220 -   train_loss = 2.796846900667463
2025-04-14 17:26:03,171 - ***** Epoch: 39: Eval results *****
2025-04-14 17:26:03,171 -   train_loss = 2.7873359407697404
2025-04-14 17:27:36,817 - ***** Epoch: 40: Eval results *****
2025-04-14 17:27:36,818 -   train_loss = 2.7771054714445085
2025-04-14 17:29:12,249 - ***** Epoch: 41: Eval results *****
2025-04-14 17:29:12,250 -   train_loss = 2.7680342083885554
2025-04-14 17:30:50,126 - ***** Epoch: 42: Eval results *****
2025-04-14 17:30:50,126 -   train_loss = 2.7620426775917175
2025-04-14 17:32:25,148 - ***** Epoch: 43: Eval results *****
2025-04-14 17:32:25,149 -   train_loss = 2.7491247161986334
2025-04-14 17:33:58,675 - ***** Epoch: 44: Eval results *****
2025-04-14 17:33:58,675 -   train_loss = 2.7427176975068592
2025-04-14 17:35:32,972 - ***** Epoch: 45: Eval results *****
2025-04-14 17:35:32,972 -   train_loss = 2.7371881689344133
2025-04-14 17:37:06,723 - ***** Epoch: 46: Eval results *****
2025-04-14 17:37:06,723 -   train_loss = 2.724003795593504
2025-04-14 17:38:41,644 - ***** Epoch: 47: Eval results *****
2025-04-14 17:38:41,644 -   train_loss = 2.7258210409255255
2025-04-14 17:40:15,099 - ***** Epoch: 48: Eval results *****
2025-04-14 17:40:15,099 -   train_loss = 2.7168221625070723
2025-04-14 17:41:49,162 - ***** Epoch: 49: Eval results *****
2025-04-14 17:41:49,162 -   train_loss = 2.7058545880847507
2025-04-14 17:43:23,372 - ***** Epoch: 50: Eval results *****
2025-04-14 17:43:23,372 -   train_loss = 2.708374992249504
2025-04-14 17:44:57,219 - ***** Epoch: 51: Eval results *****
2025-04-14 17:44:57,219 -   train_loss = 2.7042256499093678
2025-04-14 17:46:33,389 - ***** Epoch: 52: Eval results *****
2025-04-14 17:46:33,390 -   train_loss = 2.6908361779318914
2025-04-14 17:48:11,524 - ***** Epoch: 53: Eval results *****
2025-04-14 17:48:11,525 -   train_loss = 2.689374738269382
2025-04-14 17:49:48,463 - ***** Epoch: 54: Eval results *****
2025-04-14 17:49:48,463 -   train_loss = 2.683369583553738
2025-04-14 17:51:24,122 - ***** Epoch: 55: Eval results *****
2025-04-14 17:51:24,122 -   train_loss = 2.67304699950748
2025-04-14 17:52:58,656 - ***** Epoch: 56: Eval results *****
2025-04-14 17:52:58,656 -   train_loss = 2.6747824918656122
2025-04-14 17:54:33,249 - ***** Epoch: 57: Eval results *****
2025-04-14 17:54:33,249 -   train_loss = 2.6696141958236694
2025-04-14 17:56:08,777 - ***** Epoch: 58: Eval results *****
2025-04-14 17:56:08,777 -   train_loss = 2.6609748526225014
2025-04-14 17:57:45,397 - ***** Epoch: 59: Eval results *****
2025-04-14 17:57:45,397 -   train_loss = 2.6574773201866755
2025-04-14 17:59:20,382 - ***** Epoch: 60: Eval results *****
2025-04-14 17:59:20,382 -   train_loss = 2.655095573455568
2025-04-14 18:00:55,786 - ***** Epoch: 61: Eval results *****
2025-04-14 18:00:55,786 -   train_loss = 2.6514297931913346
2025-04-14 18:02:30,986 - ***** Epoch: 62: Eval results *****
2025-04-14 18:02:30,987 -   train_loss = 2.642998596978566
2025-04-14 18:04:07,065 - ***** Epoch: 63: Eval results *****
2025-04-14 18:04:07,066 -   train_loss = 2.648290236790975
2025-04-14 18:05:43,928 - ***** Epoch: 64: Eval results *****
2025-04-14 18:05:43,928 -   train_loss = 2.64188658055805
2025-04-14 18:07:20,281 - ***** Epoch: 65: Eval results *****
2025-04-14 18:07:20,282 -   train_loss = 2.6383911106321545
2025-04-14 18:08:55,452 - ***** Epoch: 66: Eval results *****
2025-04-14 18:08:55,453 -   train_loss = 2.637710843767439
2025-04-14 18:10:29,647 - ***** Epoch: 67: Eval results *****
2025-04-14 18:10:29,647 -   train_loss = 2.6305053442243547
2025-04-14 18:12:05,408 - ***** Epoch: 68: Eval results *****
2025-04-14 18:12:05,409 -   train_loss = 2.6257418677920388
2025-04-14 18:13:40,390 - ***** Epoch: 69: Eval results *****
2025-04-14 18:13:40,390 -   train_loss = 2.627586192554898
2025-04-14 18:15:14,471 - ***** Epoch: 70: Eval results *****
2025-04-14 18:15:14,472 -   train_loss = 2.6246586470376876
2025-04-14 18:16:48,410 - ***** Epoch: 71: Eval results *****
2025-04-14 18:16:48,410 -   train_loss = 2.6227263420347184
2025-04-14 18:18:22,713 - ***** Epoch: 72: Eval results *****
2025-04-14 18:18:22,713 -   train_loss = 2.6169398814912825
2025-04-14 18:19:56,682 - ***** Epoch: 73: Eval results *****
2025-04-14 18:19:56,682 -   train_loss = 2.612886601024204
2025-04-14 18:21:27,966 - ***** Epoch: 74: Eval results *****
2025-04-14 18:21:27,966 -   train_loss = 2.613869216707018
2025-04-14 18:23:06,773 - ***** Epoch: 75: Eval results *****
2025-04-14 18:23:06,773 -   train_loss = 2.6087161397177074
2025-04-14 18:24:47,161 - ***** Epoch: 76: Eval results *****
2025-04-14 18:24:47,162 -   train_loss = 2.612846643205673
2025-04-14 18:26:27,719 - ***** Epoch: 77: Eval results *****
2025-04-14 18:26:27,720 -   train_loss = 2.6055994469021995
2025-04-14 18:28:04,788 - ***** Epoch: 78: Eval results *****
2025-04-14 18:28:04,789 -   train_loss = 2.6057358526048207
2025-04-14 18:29:39,932 - ***** Epoch: 79: Eval results *****
2025-04-14 18:29:39,933 -   train_loss = 2.6018612271263484
2025-04-14 18:31:14,702 - ***** Epoch: 80: Eval results *****
2025-04-14 18:31:14,702 -   train_loss = 2.5992886092927723
2025-04-14 18:32:49,205 - ***** Epoch: 81: Eval results *****
2025-04-14 18:32:49,205 -   train_loss = 2.602423465441144
2025-04-14 18:34:22,828 - ***** Epoch: 82: Eval results *****
2025-04-14 18:34:22,828 -   train_loss = 2.5992521028670055
2025-04-14 18:35:56,722 - ***** Epoch: 83: Eval results *****
2025-04-14 18:35:56,722 -   train_loss = 2.5952301725508673
2025-04-14 18:37:31,532 - ***** Epoch: 84: Eval results *****
2025-04-14 18:37:31,533 -   train_loss = 2.5919165516656544
2025-04-14 18:39:07,185 - ***** Epoch: 85: Eval results *****
2025-04-14 18:39:07,186 -   train_loss = 2.595077541139391
2025-04-14 18:40:42,417 - ***** Epoch: 86: Eval results *****
2025-04-14 18:40:42,417 -   train_loss = 2.5873665336578613
2025-04-14 18:42:17,169 - ***** Epoch: 87: Eval results *****
2025-04-14 18:42:17,169 -   train_loss = 2.596055208690583
2025-04-14 18:43:52,149 - ***** Epoch: 88: Eval results *****
2025-04-14 18:43:52,150 -   train_loss = 2.5897079762958346
2025-04-14 18:45:27,518 - ***** Epoch: 89: Eval results *****
2025-04-14 18:45:27,519 -   train_loss = 2.5924989401348055
2025-04-14 18:47:02,852 - ***** Epoch: 90: Eval results *****
2025-04-14 18:47:02,852 -   train_loss = 2.5835092181251165
2025-04-14 18:48:37,177 - ***** Epoch: 91: Eval results *****
2025-04-14 18:48:37,178 -   train_loss = 2.5913433120364235
2025-04-14 18:50:11,596 - ***** Epoch: 92: Eval results *****
2025-04-14 18:50:11,596 -   train_loss = 2.5862625023675343
2025-04-14 18:51:45,229 - ***** Epoch: 93: Eval results *****
2025-04-14 18:51:45,229 -   train_loss = 2.586423805781773
2025-04-14 18:53:18,171 - ***** Epoch: 94: Eval results *****
2025-04-14 18:53:18,172 -   train_loss = 2.5843026316355147
2025-04-14 18:54:52,144 - ***** Epoch: 95: Eval results *****
2025-04-14 18:54:52,144 -   train_loss = 2.5925156067288113
2025-04-14 18:56:25,934 - ***** Epoch: 96: Eval results *****
2025-04-14 18:56:25,934 -   train_loss = 2.5881659889978077
2025-04-14 18:57:51,501 - ***** Epoch: 97: Eval results *****
2025-04-14 18:57:51,501 -   train_loss = 2.5839180586830017
2025-04-14 18:59:17,000 - ***** Epoch: 98: Eval results *****
2025-04-14 18:59:17,000 -   train_loss = 2.590815133518643
2025-04-14 19:00:41,988 - ***** Epoch: 99: Eval results *****
2025-04-14 19:00:41,989 -   train_loss = 2.5833921508183555
2025-04-14 19:02:07,234 - ***** Epoch: 100: Eval results *****
2025-04-14 19:02:07,234 -   train_loss = 2.5823409103211903
2025-04-14 19:02:07,797 - Pre-training finished...
2025-04-14 19:02:08,143 - Freeze all parameters but the last layer for efficiency
2025-04-14 19:02:08,153 - Multimodal Intent Recognition begins...
2025-04-14 19:02:08,153 - Training begins...
2025-04-14 19:02:54,000 - Initializing centroids with K-means++...
2025-04-14 19:02:54,209 - K-means++ used 0.21 s
2025-04-14 19:05:09,589 - K-means used 0.07 s
2025-04-14 19:05:13,436 - ***** Epoch: 1 *****
2025-04-14 19:05:13,437 - Supervised Training Loss: 5.568970
2025-04-14 19:05:13,437 - Unsupervised Training Loss: 5.710710
2025-04-14 19:07:22,526 - K-means used 0.15 s
2025-04-14 19:07:26,582 - ***** Epoch: 2 *****
2025-04-14 19:07:26,583 - Supervised Training Loss: 5.637860
2025-04-14 19:07:26,583 - Unsupervised Training Loss: 5.684280
2025-04-14 19:09:52,884 - K-means used 0.05 s
2025-04-14 19:09:57,711 - ***** Epoch: 3 *****
2025-04-14 19:09:57,711 - Supervised Training Loss: 5.660140
2025-04-14 19:09:57,712 - Unsupervised Training Loss: 5.717380
2025-04-14 19:12:22,541 - K-means used 0.06 s
2025-04-14 19:12:28,520 - ***** Epoch: 4 *****
2025-04-14 19:12:28,520 - Supervised Training Loss: 5.681000
2025-04-14 19:12:28,520 - Unsupervised Training Loss: 5.711900
2025-04-14 19:14:48,145 - K-means used 0.05 s
2025-04-14 19:14:54,761 - ***** Epoch: 5 *****
2025-04-14 19:14:54,761 - Supervised Training Loss: 5.694340
2025-04-14 19:14:54,761 - Unsupervised Training Loss: 5.706480
2025-04-14 19:17:11,526 - K-means used 0.05 s
2025-04-14 19:17:19,252 - ***** Epoch: 6 *****
2025-04-14 19:17:19,252 - Supervised Training Loss: 5.701780
2025-04-14 19:17:19,253 - Unsupervised Training Loss: 5.701850
2025-04-14 19:19:35,103 - K-means used 0.06 s
2025-04-14 19:19:44,175 - ***** Epoch: 7 *****
2025-04-14 19:19:44,176 - Supervised Training Loss: 5.708480
2025-04-14 19:19:44,176 - Unsupervised Training Loss: 5.693650
2025-04-14 19:22:17,052 - K-means used 0.05 s
2025-04-14 19:22:27,565 - ***** Epoch: 8 *****
2025-04-14 19:22:27,565 - Supervised Training Loss: 5.604100
2025-04-14 19:22:27,565 - Unsupervised Training Loss: 5.683720
2025-04-14 19:24:54,690 - K-means used 0.06 s
2025-04-14 19:25:06,998 - ***** Epoch: 9 *****
2025-04-14 19:25:06,998 - Supervised Training Loss: 5.657680
2025-04-14 19:25:06,998 - Unsupervised Training Loss: 5.666260
