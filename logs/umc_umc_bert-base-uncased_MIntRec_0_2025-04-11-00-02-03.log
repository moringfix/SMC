2025-04-11 00:02:03,234 - ============================== Params ==============================
2025-04-11 00:02:03,234 - logger_name: umc_umc_bert-base-uncased_MIntRec_0
2025-04-11 00:02:03,234 - dataset: MIntRec
2025-04-11 00:02:03,234 - multimodal_method: umc
2025-04-11 00:02:03,234 - method: umc
2025-04-11 00:02:03,234 - text_backbone: bert-base-uncased
2025-04-11 00:02:03,234 - seed: 0
2025-04-11 00:02:03,234 - num_workers: 16
2025-04-11 00:02:03,234 - log_id: umc_umc_bert-base-uncased_MIntRec_0_2025-04-11-00-02-03
2025-04-11 00:02:03,234 - gpu_id: 0
2025-04-11 00:02:03,234 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-11 00:02:03,234 - train: True
2025-04-11 00:02:03,235 - tune: True
2025-04-11 00:02:03,235 - save_model: True
2025-04-11 00:02:03,235 - save_results: True
2025-04-11 00:02:03,235 - log_path: logs
2025-04-11 00:02:03,235 - cache_path: cache
2025-04-11 00:02:03,235 - video_data_path: video_data
2025-04-11 00:02:03,235 - audio_data_path: audio_data
2025-04-11 00:02:03,235 - video_feats_path: swin_feats.pkl
2025-04-11 00:02:03,235 - audio_feats_path: wavlm_feats.pkl
2025-04-11 00:02:03,235 - results_path: results
2025-04-11 00:02:03,235 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec
2025-04-11 00:02:03,235 - model_path: models
2025-04-11 00:02:03,235 - config_file_name: umc_MIntRec
2025-04-11 00:02:03,235 - results_file_name: results_umc.csv
2025-04-11 00:02:03,235 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-11 00:02:03,235 - text_seq_len: 30
2025-04-11 00:02:03,236 - video_seq_len: 230
2025-04-11 00:02:03,236 - audio_seq_len: 480
2025-04-11 00:02:03,236 - text_feat_dim: 768
2025-04-11 00:02:03,236 - video_feat_dim: 1024
2025-04-11 00:02:03,236 - audio_feat_dim: 768
2025-04-11 00:02:03,236 - num_labels: 20
2025-04-11 00:02:03,236 - num_train_examples: 1779
2025-04-11 00:02:03,236 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-11 00:02:03,236 - pretrain_batch_size: 128
2025-04-11 00:02:03,236 - train_batch_size: 128
2025-04-11 00:02:03,236 - eval_batch_size: 128
2025-04-11 00:02:03,236 - test_batch_size: 128
2025-04-11 00:02:03,236 - num_pretrain_epochs: 100
2025-04-11 00:02:03,236 - num_train_epochs: 100
2025-04-11 00:02:03,236 - pretrain: [True]
2025-04-11 00:02:03,236 - aligned_method: ctc
2025-04-11 00:02:03,236 - need_aligned: False
2025-04-11 00:02:03,236 - freeze_pretrain_bert_parameters: [True]
2025-04-11 00:02:03,236 - freeze_train_bert_parameters: [True]
2025-04-11 00:02:03,236 - pretrain_temperature: [0.2]
2025-04-11 00:02:03,236 - train_temperature_sup: [1.4]
2025-04-11 00:02:03,236 - train_temperature_unsup: [1]
2025-04-11 00:02:03,236 - activation: tanh
2025-04-11 00:02:03,236 - lr_pre: 2e-05
2025-04-11 00:02:03,236 - lr: [0.0003]
2025-04-11 00:02:03,236 - delta: [0.05]
2025-04-11 00:02:03,237 - thres: [0.1]
2025-04-11 00:02:03,237 - topk: [5]
2025-04-11 00:02:03,237 - weight_decay: 0.01
2025-04-11 00:02:03,237 - feat_dim: 768
2025-04-11 00:02:03,237 - hidden_size: 768
2025-04-11 00:02:03,237 - grad_clip: -1.0
2025-04-11 00:02:03,237 - warmup_proportion: 0.5
2025-04-11 00:02:03,237 - hidden_dropout_prob: 0.1
2025-04-11 00:02:03,237 - weight: 1.0
2025-04-11 00:02:03,237 - loss_mode: rdrop
2025-04-11 00:02:03,237 - base_dim: 256
2025-04-11 00:02:03,237 - nheads: 8
2025-04-11 00:02:03,237 - attn_dropout: 0.1
2025-04-11 00:02:03,237 - relu_dropout: 0.1
2025-04-11 00:02:03,237 - embed_dropout: 0.1
2025-04-11 00:02:03,237 - res_dropout: 0.0
2025-04-11 00:02:03,237 - attn_mask: True
2025-04-11 00:02:03,237 - encoder_layers_1: 1
2025-04-11 00:02:03,237 - fusion_act: tanh
2025-04-11 00:02:03,237 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec/umc_umc_MIntRec_bert-base-uncased_0
2025-04-11 00:02:03,237 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec/umc_umc_MIntRec_bert-base-uncased_0/models
2025-04-11 00:02:03,237 - ============================== End Params ==============================
2025-04-11 00:02:04,291 - Freeze all parameters but the last layer for efficiency
2025-04-11 00:02:04,325 - Pre-training start...
2025-04-11 00:02:19,230 - ***** Epoch: 1: Eval results *****
2025-04-11 00:02:19,231 -   train_loss = 5.938566105706351
2025-04-11 00:02:35,469 - ***** Epoch: 2: Eval results *****
2025-04-11 00:02:35,469 -   train_loss = 5.938869510378156
2025-04-11 00:02:52,644 - ***** Epoch: 3: Eval results *****
2025-04-11 00:02:52,644 -   train_loss = 5.936440093176706
2025-04-11 00:03:10,319 - ***** Epoch: 4: Eval results *****
2025-04-11 00:03:10,320 -   train_loss = 5.937433481216431
2025-04-11 00:03:26,266 - ***** Epoch: 5: Eval results *****
2025-04-11 00:03:26,267 -   train_loss = 5.931738955633981
2025-04-11 00:03:40,601 - ***** Epoch: 6: Eval results *****
2025-04-11 00:03:40,601 -   train_loss = 5.9349009309496195
2025-04-11 00:03:54,768 - ***** Epoch: 7: Eval results *****
2025-04-11 00:03:54,769 -   train_loss = 5.931076151984079
2025-04-11 00:04:07,825 - ***** Epoch: 8: Eval results *****
2025-04-11 00:04:07,826 -   train_loss = 5.925566639219012
2025-04-11 00:04:21,214 - ***** Epoch: 9: Eval results *****
2025-04-11 00:04:21,215 -   train_loss = 5.924979414258685
2025-04-11 00:04:35,169 - ***** Epoch: 10: Eval results *****
2025-04-11 00:04:35,169 -   train_loss = 5.916105781282697
2025-04-11 00:04:49,311 - ***** Epoch: 11: Eval results *****
2025-04-11 00:04:49,311 -   train_loss = 5.908190829413278
2025-04-11 00:05:04,054 - ***** Epoch: 12: Eval results *****
2025-04-11 00:05:04,054 -   train_loss = 5.88657191821507
2025-04-11 00:05:19,038 - ***** Epoch: 13: Eval results *****
2025-04-11 00:05:19,038 -   train_loss = 5.859286069869995
2025-04-11 00:05:33,285 - ***** Epoch: 14: Eval results *****
2025-04-11 00:05:33,285 -   train_loss = 5.802095379148211
2025-04-11 00:05:46,877 - ***** Epoch: 15: Eval results *****
2025-04-11 00:05:46,877 -   train_loss = 5.704392262867519
2025-04-11 00:06:00,192 - ***** Epoch: 16: Eval results *****
2025-04-11 00:06:00,192 -   train_loss = 5.519727161952427
2025-04-11 00:06:15,303 - ***** Epoch: 17: Eval results *****
2025-04-11 00:06:15,303 -   train_loss = 5.258873156138828
2025-04-11 00:06:29,641 - ***** Epoch: 18: Eval results *****
2025-04-11 00:06:29,641 -   train_loss = 5.0045793397086005
2025-04-11 00:06:43,396 - ***** Epoch: 19: Eval results *****
2025-04-11 00:06:43,396 -   train_loss = 4.779599530356271
2025-04-11 00:06:57,502 - ***** Epoch: 20: Eval results *****
2025-04-11 00:06:57,502 -   train_loss = 4.573584284101214
2025-04-11 00:07:10,386 - ***** Epoch: 21: Eval results *****
2025-04-11 00:07:10,386 -   train_loss = 4.420046295438494
2025-04-11 00:07:23,318 - ***** Epoch: 22: Eval results *****
2025-04-11 00:07:23,318 -   train_loss = 4.267270156315395
2025-04-11 00:07:36,230 - ***** Epoch: 23: Eval results *****
2025-04-11 00:07:36,230 -   train_loss = 4.146985105105808
2025-04-11 00:07:49,294 - ***** Epoch: 24: Eval results *****
2025-04-11 00:07:49,295 -   train_loss = 4.011781420026507
2025-04-11 00:08:02,280 - ***** Epoch: 25: Eval results *****
2025-04-11 00:08:02,281 -   train_loss = 3.8979326486587524
2025-04-11 00:08:15,221 - ***** Epoch: 26: Eval results *****
2025-04-11 00:08:15,222 -   train_loss = 3.7848618371146068
2025-04-11 00:08:28,236 - ***** Epoch: 27: Eval results *****
2025-04-11 00:08:28,236 -   train_loss = 3.686783160482134
2025-04-11 00:08:41,446 - ***** Epoch: 28: Eval results *****
2025-04-11 00:08:41,447 -   train_loss = 3.588642988886152
2025-04-11 00:08:54,630 - ***** Epoch: 29: Eval results *****
2025-04-11 00:08:54,630 -   train_loss = 3.521352001598903
2025-04-11 00:09:07,358 - ***** Epoch: 30: Eval results *****
2025-04-11 00:09:07,358 -   train_loss = 3.458432146481105
2025-04-11 00:09:20,265 - ***** Epoch: 31: Eval results *****
2025-04-11 00:09:20,265 -   train_loss = 3.4014580079487393
2025-04-11 00:09:33,482 - ***** Epoch: 32: Eval results *****
2025-04-11 00:09:33,483 -   train_loss = 3.3546807936259677
2025-04-11 00:09:46,604 - ***** Epoch: 33: Eval results *****
2025-04-11 00:09:46,604 -   train_loss = 3.3100502831595287
2025-04-11 00:09:59,922 - ***** Epoch: 34: Eval results *****
2025-04-11 00:09:59,922 -   train_loss = 3.2720650604793002
2025-04-11 00:10:13,867 - ***** Epoch: 35: Eval results *****
2025-04-11 00:10:13,868 -   train_loss = 3.248613544872829
2025-04-11 00:10:27,723 - ***** Epoch: 36: Eval results *****
2025-04-11 00:10:27,723 -   train_loss = 3.1974771193095615
2025-04-11 00:10:41,348 - ***** Epoch: 37: Eval results *****
2025-04-11 00:10:41,348 -   train_loss = 3.166203107152666
2025-04-11 00:10:54,567 - ***** Epoch: 38: Eval results *****
2025-04-11 00:10:54,567 -   train_loss = 3.13033698286329
2025-04-11 00:11:07,994 - ***** Epoch: 39: Eval results *****
2025-04-11 00:11:07,995 -   train_loss = 3.112589342253549
2025-04-11 00:11:21,430 - ***** Epoch: 40: Eval results *****
2025-04-11 00:11:21,430 -   train_loss = 3.081941774913243
2025-04-11 00:11:34,629 - ***** Epoch: 41: Eval results *****
2025-04-11 00:11:34,629 -   train_loss = 3.062507050377982
2025-04-11 00:11:47,626 - ***** Epoch: 42: Eval results *****
2025-04-11 00:11:47,626 -   train_loss = 3.039598618234907
2025-04-11 00:12:00,741 - ***** Epoch: 43: Eval results *****
2025-04-11 00:12:00,741 -   train_loss = 3.012202739715576
2025-04-11 00:12:14,026 - ***** Epoch: 44: Eval results *****
2025-04-11 00:12:14,026 -   train_loss = 2.987817576953343
2025-04-11 00:12:27,478 - ***** Epoch: 45: Eval results *****
2025-04-11 00:12:27,479 -   train_loss = 2.9659398112978255
2025-04-11 00:12:40,390 - ***** Epoch: 46: Eval results *****
2025-04-11 00:12:40,390 -   train_loss = 2.9421326432909285
2025-04-11 00:12:53,624 - ***** Epoch: 47: Eval results *****
2025-04-11 00:12:53,624 -   train_loss = 2.9213895286832536
2025-04-11 00:13:06,604 - ***** Epoch: 48: Eval results *****
2025-04-11 00:13:06,604 -   train_loss = 2.902592965534755
2025-04-11 00:13:18,995 - ***** Epoch: 49: Eval results *****
2025-04-11 00:13:18,995 -   train_loss = 2.8741620097841536
2025-04-11 00:13:31,868 - ***** Epoch: 50: Eval results *****
2025-04-11 00:13:31,868 -   train_loss = 2.8567055463790894
2025-04-11 00:13:44,438 - ***** Epoch: 51: Eval results *****
2025-04-11 00:13:44,438 -   train_loss = 2.8606476272855486
2025-04-11 00:13:57,190 - ***** Epoch: 52: Eval results *****
2025-04-11 00:13:57,191 -   train_loss = 2.8307906729834422
2025-04-11 00:14:09,708 - ***** Epoch: 53: Eval results *****
2025-04-11 00:14:09,709 -   train_loss = 2.81762501171657
2025-04-11 00:14:22,586 - ***** Epoch: 54: Eval results *****
2025-04-11 00:14:22,586 -   train_loss = 2.814410788672311
2025-04-11 00:14:35,317 - ***** Epoch: 55: Eval results *****
2025-04-11 00:14:35,318 -   train_loss = 2.800091505050659
2025-04-11 00:14:47,994 - ***** Epoch: 56: Eval results *****
2025-04-11 00:14:47,994 -   train_loss = 2.776562980243138
2025-04-11 00:15:00,497 - ***** Epoch: 57: Eval results *****
2025-04-11 00:15:00,497 -   train_loss = 2.7695784057889665
2025-04-11 00:15:13,330 - ***** Epoch: 58: Eval results *****
2025-04-11 00:15:13,330 -   train_loss = 2.7572168622698103
2025-04-11 00:15:26,066 - ***** Epoch: 59: Eval results *****
2025-04-11 00:15:26,066 -   train_loss = 2.749739101954869
2025-04-11 00:15:38,806 - ***** Epoch: 60: Eval results *****
2025-04-11 00:15:38,806 -   train_loss = 2.7360059363501414
2025-04-11 00:15:51,211 - ***** Epoch: 61: Eval results *****
2025-04-11 00:15:51,212 -   train_loss = 2.7341276918138777
2025-04-11 00:16:04,020 - ***** Epoch: 62: Eval results *****
2025-04-11 00:16:04,021 -   train_loss = 2.7204280580793108
2025-04-11 00:16:17,200 - ***** Epoch: 63: Eval results *****
2025-04-11 00:16:17,200 -   train_loss = 2.724720937865121
2025-04-11 00:16:30,345 - ***** Epoch: 64: Eval results *****
2025-04-11 00:16:30,346 -   train_loss = 2.6995548009872437
2025-04-11 00:16:43,226 - ***** Epoch: 65: Eval results *****
2025-04-11 00:16:43,226 -   train_loss = 2.706212571689061
2025-04-11 00:16:56,490 - ***** Epoch: 66: Eval results *****
2025-04-11 00:16:56,491 -   train_loss = 2.6999261719839915
2025-04-11 00:17:09,505 - ***** Epoch: 67: Eval results *****
2025-04-11 00:17:09,505 -   train_loss = 2.679860063961574
2025-04-11 00:17:22,329 - ***** Epoch: 68: Eval results *****
2025-04-11 00:17:22,329 -   train_loss = 2.6836276735578264
2025-04-11 00:17:35,823 - ***** Epoch: 69: Eval results *****
2025-04-11 00:17:35,824 -   train_loss = 2.673388259751456
2025-04-11 00:17:49,565 - ***** Epoch: 70: Eval results *****
2025-04-11 00:17:49,566 -   train_loss = 2.6795576129640852
2025-04-11 00:18:02,889 - ***** Epoch: 71: Eval results *****
2025-04-11 00:18:02,890 -   train_loss = 2.677729163851057
2025-04-11 00:18:15,892 - ***** Epoch: 72: Eval results *****
2025-04-11 00:18:15,892 -   train_loss = 2.6739875418799266
2025-04-11 00:18:28,524 - ***** Epoch: 73: Eval results *****
2025-04-11 00:18:28,524 -   train_loss = 2.6696034329278127
2025-04-11 00:18:41,661 - ***** Epoch: 74: Eval results *****
2025-04-11 00:18:41,661 -   train_loss = 2.6684571163994923
2025-04-11 00:18:54,464 - ***** Epoch: 75: Eval results *****
2025-04-11 00:18:54,465 -   train_loss = 2.667287383760725
2025-04-11 00:19:07,152 - ***** Epoch: 76: Eval results *****
2025-04-11 00:19:07,153 -   train_loss = 2.6506716864449635
2025-04-11 00:19:19,824 - ***** Epoch: 77: Eval results *****
2025-04-11 00:19:19,824 -   train_loss = 2.6437223809106007
2025-04-11 00:19:32,813 - ***** Epoch: 78: Eval results *****
2025-04-11 00:19:32,814 -   train_loss = 2.643050483294896
2025-04-11 00:19:45,517 - ***** Epoch: 79: Eval results *****
2025-04-11 00:19:45,517 -   train_loss = 2.641696606363569
2025-04-11 00:19:58,185 - ***** Epoch: 80: Eval results *****
2025-04-11 00:19:58,185 -   train_loss = 2.642590284347534
2025-04-11 00:20:10,971 - ***** Epoch: 81: Eval results *****
2025-04-11 00:20:10,971 -   train_loss = 2.639065214565822
2025-04-11 00:20:23,960 - ***** Epoch: 82: Eval results *****
2025-04-11 00:20:23,960 -   train_loss = 2.6348000935145786
2025-04-11 00:20:38,089 - ***** Epoch: 83: Eval results *****
2025-04-11 00:20:38,090 -   train_loss = 2.6469177177974155
2025-04-11 00:20:52,140 - ***** Epoch: 84: Eval results *****
2025-04-11 00:20:52,140 -   train_loss = 2.6326355082648143
2025-04-11 00:21:05,924 - ***** Epoch: 85: Eval results *****
2025-04-11 00:21:05,924 -   train_loss = 2.6341098887579784
2025-04-11 00:21:19,054 - ***** Epoch: 86: Eval results *****
2025-04-11 00:21:19,055 -   train_loss = 2.6307363169533864
2025-04-11 00:21:31,666 - ***** Epoch: 87: Eval results *****
2025-04-11 00:21:31,666 -   train_loss = 2.629253472600664
2025-04-11 00:21:44,617 - ***** Epoch: 88: Eval results *****
2025-04-11 00:21:44,618 -   train_loss = 2.628473469189235
2025-04-11 00:21:57,114 - ***** Epoch: 89: Eval results *****
2025-04-11 00:21:57,115 -   train_loss = 2.6241971254348755
2025-04-11 00:22:10,015 - ***** Epoch: 90: Eval results *****
2025-04-11 00:22:10,016 -   train_loss = 2.628428816795349
2025-04-11 00:22:22,350 - ***** Epoch: 91: Eval results *****
2025-04-11 00:22:22,350 -   train_loss = 2.623288631439209
2025-04-11 00:22:35,189 - ***** Epoch: 92: Eval results *****
2025-04-11 00:22:35,189 -   train_loss = 2.6215842621667043
2025-04-11 00:22:47,758 - ***** Epoch: 93: Eval results *****
2025-04-11 00:22:47,758 -   train_loss = 2.6233344418661937
2025-04-11 00:23:00,473 - ***** Epoch: 94: Eval results *****
2025-04-11 00:23:00,474 -   train_loss = 2.622926184109279
2025-04-11 00:23:12,870 - ***** Epoch: 95: Eval results *****
2025-04-11 00:23:12,870 -   train_loss = 2.622331142425537
2025-04-11 00:23:25,725 - ***** Epoch: 96: Eval results *****
2025-04-11 00:23:25,725 -   train_loss = 2.6222749097006663
2025-04-11 00:23:38,277 - ***** Epoch: 97: Eval results *****
2025-04-11 00:23:38,277 -   train_loss = 2.6245990140097484
2025-04-11 00:23:50,924 - ***** Epoch: 98: Eval results *****
2025-04-11 00:23:50,924 -   train_loss = 2.6319937705993652
2025-04-11 00:24:03,267 - ***** Epoch: 99: Eval results *****
2025-04-11 00:24:03,267 -   train_loss = 2.6161746638161794
2025-04-11 00:24:15,861 - ***** Epoch: 100: Eval results *****
2025-04-11 00:24:15,861 -   train_loss = 2.6204133374350413
2025-04-11 00:24:17,586 - Pre-training finished...
2025-04-11 00:24:17,856 - Freeze all parameters but the last layer for efficiency
2025-04-11 00:24:17,866 - Multimodal Intent Recognition begins...
2025-04-11 00:24:17,866 - Training begins...
2025-04-11 00:24:36,678 - Initializing centroids with K-means++...
