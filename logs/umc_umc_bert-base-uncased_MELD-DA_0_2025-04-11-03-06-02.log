2025-04-11 03:06:02,034 - ============================== Params ==============================
2025-04-11 03:06:02,034 - logger_name: umc_umc_bert-base-uncased_MELD-DA_0
2025-04-11 03:06:02,034 - dataset: MELD-DA
2025-04-11 03:06:02,035 - multimodal_method: umc
2025-04-11 03:06:02,035 - method: umc
2025-04-11 03:06:02,035 - text_backbone: bert-base-uncased
2025-04-11 03:06:02,035 - seed: 0
2025-04-11 03:06:02,035 - num_workers: 16
2025-04-11 03:06:02,035 - log_id: umc_umc_bert-base-uncased_MELD-DA_0_2025-04-11-03-06-02
2025-04-11 03:06:02,035 - gpu_id: 1
2025-04-11 03:06:02,035 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-11 03:06:02,035 - train: True
2025-04-11 03:06:02,035 - tune: True
2025-04-11 03:06:02,035 - save_model: True
2025-04-11 03:06:02,035 - save_results: True
2025-04-11 03:06:02,036 - log_path: logs
2025-04-11 03:06:02,036 - cache_path: cache
2025-04-11 03:06:02,036 - video_data_path: video_data
2025-04-11 03:06:02,036 - audio_data_path: audio_data
2025-04-11 03:06:02,036 - video_feats_path: swin_feats.pkl
2025-04-11 03:06:02,036 - audio_feats_path: wavlm_feats.pkl
2025-04-11 03:06:02,036 - results_path: results
2025-04-11 03:06:02,036 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA
2025-04-11 03:06:02,036 - model_path: models
2025-04-11 03:06:02,036 - config_file_name: umc_MELD-DA
2025-04-11 03:06:02,036 - results_file_name: results_umc.csv
2025-04-11 03:06:02,036 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-11 03:06:02,036 - text_seq_len: 70
2025-04-11 03:06:02,036 - video_seq_len: 250
2025-04-11 03:06:02,036 - audio_seq_len: 520
2025-04-11 03:06:02,036 - text_feat_dim: 768
2025-04-11 03:06:02,036 - video_feat_dim: 1024
2025-04-11 03:06:02,036 - audio_feat_dim: 768
2025-04-11 03:06:02,036 - num_labels: 12
2025-04-11 03:06:02,036 - num_train_examples: 7990
2025-04-11 03:06:02,036 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-11 03:06:02,036 - pretrain_batch_size: 128
2025-04-11 03:06:02,036 - train_batch_size: 128
2025-04-11 03:06:02,037 - eval_batch_size: 128
2025-04-11 03:06:02,037 - test_batch_size: 128
2025-04-11 03:06:02,037 - num_pretrain_epochs: [100]
2025-04-11 03:06:02,037 - num_train_epochs: [100]
2025-04-11 03:06:02,037 - pretrain: [True]
2025-04-11 03:06:02,037 - aligned_method: ctc
2025-04-11 03:06:02,037 - need_aligned: False
2025-04-11 03:06:02,037 - freeze_pretrain_bert_parameters: True
2025-04-11 03:06:02,037 - freeze_train_bert_parameters: True
2025-04-11 03:06:02,037 - pretrain_temperature: [0.2]
2025-04-11 03:06:02,037 - train_temperature_sup: [20]
2025-04-11 03:06:02,037 - train_temperature_unsup: [20]
2025-04-11 03:06:02,037 - activation: tanh
2025-04-11 03:06:02,037 - lr_pre: [5e-06]
2025-04-11 03:06:02,037 - lr: [0.0002]
2025-04-11 03:06:02,037 - delta: [0.05]
2025-04-11 03:06:02,037 - thres: [0.1]
2025-04-11 03:06:02,037 - topk: [5]
2025-04-11 03:06:02,037 - weight_decay: 0.01
2025-04-11 03:06:02,037 - feat_dim: 768
2025-04-11 03:06:02,037 - hidden_size: 768
2025-04-11 03:06:02,037 - grad_clip: [-1.0]
2025-04-11 03:06:02,037 - warmup_proportion: 0.5
2025-04-11 03:06:02,038 - hidden_dropout_prob: 0.1
2025-04-11 03:06:02,038 - weight: 1.0
2025-04-11 03:06:02,038 - loss_mode: rdrop
2025-04-11 03:06:02,038 - base_dim: [256]
2025-04-11 03:06:02,038 - nheads: 8
2025-04-11 03:06:02,038 - attn_dropout: 0.1
2025-04-11 03:06:02,038 - relu_dropout: 0.1
2025-04-11 03:06:02,038 - embed_dropout: 0.1
2025-04-11 03:06:02,038 - res_dropout: 0.0
2025-04-11 03:06:02,038 - attn_mask: True
2025-04-11 03:06:02,038 - encoder_layers_1: 1
2025-04-11 03:06:02,038 - fusion_act: tanh
2025-04-11 03:06:02,038 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0
2025-04-11 03:06:02,038 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0/models
2025-04-11 03:06:02,038 - ============================== End Params ==============================
2025-04-11 03:06:03,916 - Freeze all parameters but the last layer for efficiency
2025-04-11 03:06:03,953 - Pre-training start...
2025-04-11 03:07:43,571 - ***** Epoch: 1: Eval results *****
2025-04-11 03:07:43,572 -   train_loss = 5.912415890466599
2025-04-11 03:09:25,797 - ***** Epoch: 2: Eval results *****
2025-04-11 03:09:25,797 -   train_loss = 5.912562491401793
2025-04-11 03:11:10,566 - ***** Epoch: 3: Eval results *****
2025-04-11 03:11:10,567 -   train_loss = 5.910225898500473
2025-04-11 03:12:56,391 - ***** Epoch: 4: Eval results *****
2025-04-11 03:12:56,392 -   train_loss = 5.907440473162938
2025-04-11 03:14:49,093 - ***** Epoch: 5: Eval results *****
2025-04-11 03:14:49,094 -   train_loss = 5.903918379829044
2025-04-11 03:16:40,498 - ***** Epoch: 6: Eval results *****
2025-04-11 03:16:40,499 -   train_loss = 5.8974241680569115
2025-04-11 03:18:19,354 - ***** Epoch: 7: Eval results *****
2025-04-11 03:18:19,354 -   train_loss = 5.889200286259727
2025-04-11 03:19:57,092 - ***** Epoch: 8: Eval results *****
2025-04-11 03:19:57,092 -   train_loss = 5.875740020994156
2025-04-11 03:21:34,878 - ***** Epoch: 9: Eval results *****
2025-04-11 03:21:34,878 -   train_loss = 5.855111099424816
2025-04-11 03:23:10,757 - ***** Epoch: 10: Eval results *****
2025-04-11 03:23:10,758 -   train_loss = 5.815386249905541
2025-04-11 03:24:47,645 - ***** Epoch: 11: Eval results *****
2025-04-11 03:24:47,645 -   train_loss = 5.752287016974555
2025-04-11 03:26:24,074 - ***** Epoch: 12: Eval results *****
2025-04-11 03:26:24,074 -   train_loss = 5.649149047003852
2025-04-11 03:28:00,390 - ***** Epoch: 13: Eval results *****
2025-04-11 03:28:00,390 -   train_loss = 5.496181117163764
2025-04-11 03:29:35,554 - ***** Epoch: 14: Eval results *****
2025-04-11 03:29:35,555 -   train_loss = 5.321671289110941
2025-04-11 03:31:10,145 - ***** Epoch: 15: Eval results *****
2025-04-11 03:31:10,146 -   train_loss = 5.136494265662299
2025-04-11 03:32:42,749 - ***** Epoch: 16: Eval results *****
2025-04-11 03:32:42,749 -   train_loss = 4.938825531611367
2025-04-11 03:34:16,069 - ***** Epoch: 17: Eval results *****
2025-04-11 03:34:16,070 -   train_loss = 4.731680287255181
2025-04-11 03:35:51,513 - ***** Epoch: 18: Eval results *****
2025-04-11 03:35:51,513 -   train_loss = 4.556759497476002
2025-04-11 03:37:27,784 - ***** Epoch: 19: Eval results *****
2025-04-11 03:37:27,784 -   train_loss = 4.410900626863752
2025-04-11 03:39:03,274 - ***** Epoch: 20: Eval results *****
2025-04-11 03:39:03,274 -   train_loss = 4.277378797531128
2025-04-11 03:40:39,302 - ***** Epoch: 21: Eval results *****
2025-04-11 03:40:39,303 -   train_loss = 4.154019261163379
2025-04-11 03:42:14,944 - ***** Epoch: 22: Eval results *****
2025-04-11 03:42:14,944 -   train_loss = 4.038870523846339
2025-04-11 03:43:50,028 - ***** Epoch: 23: Eval results *****
2025-04-11 03:43:50,029 -   train_loss = 3.9401122199164496
2025-04-11 03:45:25,942 - ***** Epoch: 24: Eval results *****
2025-04-11 03:45:25,943 -   train_loss = 3.8478926325601246
2025-04-11 03:47:01,413 - ***** Epoch: 25: Eval results *****
2025-04-11 03:47:01,413 -   train_loss = 3.7583197071438743
2025-04-11 03:48:38,262 - ***** Epoch: 26: Eval results *****
2025-04-11 03:48:38,262 -   train_loss = 3.666395073845273
2025-04-11 03:50:13,676 - ***** Epoch: 27: Eval results *****
2025-04-11 03:50:13,676 -   train_loss = 3.600977693285261
2025-04-11 03:51:48,790 - ***** Epoch: 28: Eval results *****
2025-04-11 03:51:48,790 -   train_loss = 3.5355877951970176
2025-04-11 03:53:24,971 - ***** Epoch: 29: Eval results *****
2025-04-11 03:53:24,971 -   train_loss = 3.484302388297187
2025-04-11 03:55:01,326 - ***** Epoch: 30: Eval results *****
2025-04-11 03:55:01,327 -   train_loss = 3.4285766132294184
2025-04-11 03:56:38,304 - ***** Epoch: 31: Eval results *****
2025-04-11 03:56:38,304 -   train_loss = 3.386050095633855
2025-04-11 03:58:16,583 - ***** Epoch: 32: Eval results *****
2025-04-11 03:58:16,584 -   train_loss = 3.3340500612107533
2025-04-11 03:59:52,390 - ***** Epoch: 33: Eval results *****
2025-04-11 03:59:52,391 -   train_loss = 3.290429849473257
2025-04-11 04:01:27,182 - ***** Epoch: 34: Eval results *****
2025-04-11 04:01:27,183 -   train_loss = 3.2446580273764476
2025-04-11 04:03:01,309 - ***** Epoch: 35: Eval results *****
2025-04-11 04:03:01,310 -   train_loss = 3.2184081683083186
2025-04-11 04:04:35,672 - ***** Epoch: 36: Eval results *****
2025-04-11 04:04:35,673 -   train_loss = 3.1885993480682373
2025-04-11 04:06:09,348 - ***** Epoch: 37: Eval results *****
2025-04-11 04:06:09,348 -   train_loss = 3.1617172521258157
2025-04-11 04:07:42,316 - ***** Epoch: 38: Eval results *****
2025-04-11 04:07:42,316 -   train_loss = 3.1320478083595398
2025-04-11 04:09:15,388 - ***** Epoch: 39: Eval results *****
2025-04-11 04:09:15,388 -   train_loss = 3.1050596653469027
2025-04-11 04:10:48,868 - ***** Epoch: 40: Eval results *****
2025-04-11 04:10:48,869 -   train_loss = 3.0773288976578486
2025-04-11 04:12:24,526 - ***** Epoch: 41: Eval results *****
2025-04-11 04:12:24,527 -   train_loss = 3.0552903016408286
2025-04-11 04:14:00,512 - ***** Epoch: 42: Eval results *****
2025-04-11 04:14:00,513 -   train_loss = 3.0345189079405768
2025-04-11 04:15:36,630 - ***** Epoch: 43: Eval results *****
2025-04-11 04:15:36,631 -   train_loss = 3.0050377088879783
2025-04-11 04:17:12,232 - ***** Epoch: 44: Eval results *****
2025-04-11 04:17:12,232 -   train_loss = 2.9875137995159817
2025-04-11 04:18:48,369 - ***** Epoch: 45: Eval results *****
2025-04-11 04:18:48,370 -   train_loss = 2.966362612588065
2025-04-11 04:20:25,628 - ***** Epoch: 46: Eval results *****
2025-04-11 04:20:25,629 -   train_loss = 2.9426607669345914
2025-04-11 04:22:02,335 - ***** Epoch: 47: Eval results *****
2025-04-11 04:22:02,335 -   train_loss = 2.9282466873290045
2025-04-11 04:23:39,226 - ***** Epoch: 48: Eval results *****
2025-04-11 04:23:39,226 -   train_loss = 2.9061279145498125
2025-04-11 04:25:14,437 - ***** Epoch: 49: Eval results *****
2025-04-11 04:25:14,438 -   train_loss = 2.883031031442067
2025-04-11 04:26:50,018 - ***** Epoch: 50: Eval results *****
2025-04-11 04:26:50,018 -   train_loss = 2.868982099351429
2025-04-11 04:28:26,282 - ***** Epoch: 51: Eval results *****
2025-04-11 04:28:26,282 -   train_loss = 2.8575700578235446
2025-04-11 04:30:02,213 - ***** Epoch: 52: Eval results *****
2025-04-11 04:30:02,214 -   train_loss = 2.831792320523943
2025-04-11 04:31:37,743 - ***** Epoch: 53: Eval results *****
2025-04-11 04:31:37,743 -   train_loss = 2.8161126696874224
2025-04-11 04:33:13,364 - ***** Epoch: 54: Eval results *****
2025-04-11 04:33:13,364 -   train_loss = 2.8057727813720703
2025-04-11 04:34:49,610 - ***** Epoch: 55: Eval results *****
2025-04-11 04:34:49,611 -   train_loss = 2.7844954142494807
2025-04-11 04:36:25,144 - ***** Epoch: 56: Eval results *****
2025-04-11 04:36:25,145 -   train_loss = 2.782178280845521
2025-04-11 04:37:58,740 - ***** Epoch: 57: Eval results *****
2025-04-11 04:37:58,741 -   train_loss = 2.7696752850971524
2025-04-11 04:39:32,812 - ***** Epoch: 58: Eval results *****
2025-04-11 04:39:32,812 -   train_loss = 2.754293403928242
2025-04-11 04:41:06,957 - ***** Epoch: 59: Eval results *****
2025-04-11 04:41:06,958 -   train_loss = 2.7447258669232566
2025-04-11 04:42:41,564 - ***** Epoch: 60: Eval results *****
2025-04-11 04:42:41,564 -   train_loss = 2.7393489338102794
2025-04-11 04:44:15,072 - ***** Epoch: 61: Eval results *****
2025-04-11 04:44:15,073 -   train_loss = 2.7256966791455706
2025-04-11 04:45:50,721 - ***** Epoch: 62: Eval results *****
2025-04-11 04:45:50,721 -   train_loss = 2.717648362356519
2025-04-11 04:47:22,422 - ***** Epoch: 63: Eval results *****
2025-04-11 04:47:22,422 -   train_loss = 2.7067531555417985
2025-04-11 04:48:58,242 - ***** Epoch: 64: Eval results *****
2025-04-11 04:48:58,243 -   train_loss = 2.7061798913138255
2025-04-11 04:50:34,521 - ***** Epoch: 65: Eval results *****
2025-04-11 04:50:34,522 -   train_loss = 2.6941235784500366
2025-04-11 04:52:10,016 - ***** Epoch: 66: Eval results *****
2025-04-11 04:52:10,016 -   train_loss = 2.6900059533497642
2025-04-11 04:53:45,573 - ***** Epoch: 67: Eval results *****
2025-04-11 04:53:45,574 -   train_loss = 2.6809403858487566
2025-04-11 04:55:21,442 - ***** Epoch: 68: Eval results *****
2025-04-11 04:55:21,443 -   train_loss = 2.6740661689213345
2025-04-11 04:56:56,826 - ***** Epoch: 69: Eval results *****
2025-04-11 04:56:56,827 -   train_loss = 2.673009196917216
2025-04-11 04:58:33,348 - ***** Epoch: 70: Eval results *****
2025-04-11 04:58:33,349 -   train_loss = 2.6676922790587896
2025-04-11 05:00:09,866 - ***** Epoch: 71: Eval results *****
2025-04-11 05:00:09,866 -   train_loss = 2.658144640544104
2025-04-11 05:01:45,838 - ***** Epoch: 72: Eval results *****
2025-04-11 05:01:45,839 -   train_loss = 2.6505028860909596
2025-04-11 05:03:23,306 - ***** Epoch: 73: Eval results *****
2025-04-11 05:03:23,306 -   train_loss = 2.6444757892971946
2025-04-11 05:04:59,517 - ***** Epoch: 74: Eval results *****
2025-04-11 05:04:59,517 -   train_loss = 2.6476995282702975
2025-04-11 05:06:35,146 - ***** Epoch: 75: Eval results *****
2025-04-11 05:06:35,146 -   train_loss = 2.640067425985185
2025-04-11 05:08:09,706 - ***** Epoch: 76: Eval results *****
2025-04-11 05:08:09,707 -   train_loss = 2.6430105330452087
2025-04-11 05:09:45,462 - ***** Epoch: 77: Eval results *****
2025-04-11 05:09:45,462 -   train_loss = 2.638051331989349
2025-04-11 05:11:21,010 - ***** Epoch: 78: Eval results *****
2025-04-11 05:11:21,010 -   train_loss = 2.6294301937496853
2025-04-11 05:12:55,951 - ***** Epoch: 79: Eval results *****
2025-04-11 05:12:55,952 -   train_loss = 2.6228474984093317
2025-04-11 05:14:31,004 - ***** Epoch: 80: Eval results *****
2025-04-11 05:14:31,004 -   train_loss = 2.6225868879802645
2025-04-11 05:16:03,965 - ***** Epoch: 81: Eval results *****
2025-04-11 05:16:03,966 -   train_loss = 2.622007818449111
2025-04-11 05:17:38,265 - ***** Epoch: 82: Eval results *****
2025-04-11 05:17:38,265 -   train_loss = 2.6155605902747503
2025-04-11 05:19:11,914 - ***** Epoch: 83: Eval results *****
2025-04-11 05:19:11,914 -   train_loss = 2.6109219827349226
2025-04-11 05:20:46,558 - ***** Epoch: 84: Eval results *****
2025-04-11 05:20:46,558 -   train_loss = 2.60921919913519
2025-04-11 05:22:21,557 - ***** Epoch: 85: Eval results *****
2025-04-11 05:22:21,558 -   train_loss = 2.607608437538147
2025-04-11 05:23:53,622 - ***** Epoch: 86: Eval results *****
2025-04-11 05:23:53,622 -   train_loss = 2.600644300854395
2025-04-11 05:25:27,585 - ***** Epoch: 87: Eval results *****
2025-04-11 05:25:27,586 -   train_loss = 2.6070974970620777
2025-04-11 05:27:04,513 - ***** Epoch: 88: Eval results *****
2025-04-11 05:27:04,514 -   train_loss = 2.6090509721211026
2025-04-11 05:28:40,712 - ***** Epoch: 89: Eval results *****
2025-04-11 05:28:40,713 -   train_loss = 2.6011704225388783
2025-04-11 05:30:16,651 - ***** Epoch: 90: Eval results *****
2025-04-11 05:30:16,652 -   train_loss = 2.5986390851792835
2025-04-11 05:31:52,672 - ***** Epoch: 91: Eval results *****
2025-04-11 05:31:52,672 -   train_loss = 2.6015177151513478
2025-04-11 05:33:29,028 - ***** Epoch: 92: Eval results *****
2025-04-11 05:33:29,028 -   train_loss = 2.594033025559925
2025-04-11 05:35:04,685 - ***** Epoch: 93: Eval results *****
2025-04-11 05:35:04,685 -   train_loss = 2.596906648741828
2025-04-11 05:36:40,764 - ***** Epoch: 94: Eval results *****
2025-04-11 05:36:40,765 -   train_loss = 2.59555485135033
2025-04-11 05:38:17,161 - ***** Epoch: 95: Eval results *****
2025-04-11 05:38:17,161 -   train_loss = 2.597105370627509
2025-04-11 05:39:51,838 - ***** Epoch: 96: Eval results *****
2025-04-11 05:39:51,839 -   train_loss = 2.589512032175821
2025-04-11 05:41:28,654 - ***** Epoch: 97: Eval results *****
2025-04-11 05:41:28,655 -   train_loss = 2.594847185271127
2025-04-11 05:43:05,757 - ***** Epoch: 98: Eval results *****
2025-04-11 05:43:05,758 -   train_loss = 2.596835976555234
2025-04-11 05:44:41,824 - ***** Epoch: 99: Eval results *****
2025-04-11 05:44:41,824 -   train_loss = 2.5934767174342324
2025-04-11 05:46:18,050 - ***** Epoch: 100: Eval results *****
2025-04-11 05:46:18,050 -   train_loss = 2.587637608013456
2025-04-11 05:46:18,619 - Pre-training finished...
2025-04-11 05:46:19,213 - Freeze all parameters but the last layer for efficiency
2025-04-11 05:46:19,222 - Multimodal Intent Recognition begins...
2025-04-11 05:46:19,223 - Training begins...
2025-04-11 05:47:01,812 - Initializing centroids with K-means++...
2025-04-11 05:47:02,213 - K-means++ used 0.4 s
2025-04-11 05:49:22,803 - K-means used 0.25 s
2025-04-11 05:49:26,697 - ***** Epoch: 1 *****
2025-04-11 05:49:26,698 - Supervised Training Loss: 5.683930
2025-04-11 05:49:26,698 - Unsupervised Training Loss: 5.879770
2025-04-11 05:51:53,629 - K-means used 0.09 s
2025-04-11 05:51:58,134 - ***** Epoch: 2 *****
2025-04-11 05:51:58,135 - Supervised Training Loss: 5.796960
2025-04-11 05:51:58,135 - Unsupervised Training Loss: 5.864890
