2025-04-11 00:26:05,010 - ============================== Params ==============================
2025-04-11 00:26:05,010 - logger_name: umc_umc_bert-base-uncased_MELD-DA_0
2025-04-11 00:26:05,010 - dataset: MELD-DA
2025-04-11 00:26:05,011 - multimodal_method: umc
2025-04-11 00:26:05,011 - method: umc
2025-04-11 00:26:05,011 - text_backbone: bert-base-uncased
2025-04-11 00:26:05,011 - seed: 0
2025-04-11 00:26:05,011 - num_workers: 16
2025-04-11 00:26:05,011 - log_id: umc_umc_bert-base-uncased_MELD-DA_0_2025-04-11-00-26-04
2025-04-11 00:26:05,011 - gpu_id: 0
2025-04-11 00:26:05,011 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-11 00:26:05,011 - train: True
2025-04-11 00:26:05,011 - tune: True
2025-04-11 00:26:05,011 - save_model: True
2025-04-11 00:26:05,011 - save_results: True
2025-04-11 00:26:05,011 - log_path: logs
2025-04-11 00:26:05,011 - cache_path: cache
2025-04-11 00:26:05,012 - video_data_path: video_data
2025-04-11 00:26:05,012 - audio_data_path: audio_data
2025-04-11 00:26:05,012 - video_feats_path: swin_feats.pkl
2025-04-11 00:26:05,012 - audio_feats_path: wavlm_feats.pkl
2025-04-11 00:26:05,012 - results_path: results
2025-04-11 00:26:05,012 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA
2025-04-11 00:26:05,012 - model_path: models
2025-04-11 00:26:05,012 - config_file_name: umc_MELD-DA
2025-04-11 00:26:05,012 - results_file_name: results_umc.csv
2025-04-11 00:26:05,012 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-11 00:26:05,012 - text_seq_len: 70
2025-04-11 00:26:05,012 - video_seq_len: 250
2025-04-11 00:26:05,012 - audio_seq_len: 520
2025-04-11 00:26:05,012 - text_feat_dim: 768
2025-04-11 00:26:05,012 - video_feat_dim: 1024
2025-04-11 00:26:05,012 - audio_feat_dim: 768
2025-04-11 00:26:05,012 - num_labels: 12
2025-04-11 00:26:05,012 - num_train_examples: 7990
2025-04-11 00:26:05,012 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-11 00:26:05,012 - pretrain_batch_size: 128
2025-04-11 00:26:05,012 - train_batch_size: 128
2025-04-11 00:26:05,012 - eval_batch_size: 128
2025-04-11 00:26:05,012 - test_batch_size: 128
2025-04-11 00:26:05,012 - num_pretrain_epochs: [100]
2025-04-11 00:26:05,012 - num_train_epochs: [100]
2025-04-11 00:26:05,012 - pretrain: [True]
2025-04-11 00:26:05,012 - aligned_method: ctc
2025-04-11 00:26:05,012 - need_aligned: False
2025-04-11 00:26:05,012 - freeze_pretrain_bert_parameters: True
2025-04-11 00:26:05,012 - freeze_train_bert_parameters: True
2025-04-11 00:26:05,013 - pretrain_temperature: [0.2]
2025-04-11 00:26:05,013 - train_temperature_sup: [20]
2025-04-11 00:26:05,013 - train_temperature_unsup: [20]
2025-04-11 00:26:05,013 - activation: tanh
2025-04-11 00:26:05,013 - lr_pre: [2e-05]
2025-04-11 00:26:05,013 - lr: [0.0002]
2025-04-11 00:26:05,013 - delta: [0.05]
2025-04-11 00:26:05,013 - thres: [0.1]
2025-04-11 00:26:05,013 - topk: [5]
2025-04-11 00:26:05,013 - weight_decay: 0.01
2025-04-11 00:26:05,013 - feat_dim: 768
2025-04-11 00:26:05,013 - hidden_size: 768
2025-04-11 00:26:05,013 - grad_clip: [-1.0]
2025-04-11 00:26:05,013 - warmup_proportion: 0.1
2025-04-11 00:26:05,013 - hidden_dropout_prob: 0.1
2025-04-11 00:26:05,013 - weight: 1.0
2025-04-11 00:26:05,013 - loss_mode: rdrop
2025-04-11 00:26:05,013 - base_dim: [256]
2025-04-11 00:26:05,013 - nheads: 8
2025-04-11 00:26:05,013 - attn_dropout: 0.1
2025-04-11 00:26:05,013 - relu_dropout: 0.1
2025-04-11 00:26:05,013 - embed_dropout: 0.1
2025-04-11 00:26:05,013 - res_dropout: 0.0
2025-04-11 00:26:05,013 - attn_mask: True
2025-04-11 00:26:05,014 - encoder_layers_1: 1
2025-04-11 00:26:05,014 - fusion_act: tanh
2025-04-11 00:26:05,014 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0
2025-04-11 00:26:05,014 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MELD-DA/umc_umc_MELD-DA_bert-base-uncased_0/models
2025-04-11 00:26:05,014 - ============================== End Params ==============================
2025-04-11 00:26:07,279 - Freeze all parameters but the last layer for efficiency
2025-04-11 00:26:07,317 - Pre-training start...
2025-04-11 00:27:38,148 - ***** Epoch: 1: Eval results *****
2025-04-11 00:27:38,148 -   train_loss = 5.90979870538863
2025-04-11 00:29:09,968 - ***** Epoch: 2: Eval results *****
2025-04-11 00:29:09,968 -   train_loss = 5.885373047419956
2025-04-11 00:30:42,473 - ***** Epoch: 3: Eval results *****
2025-04-11 00:30:42,474 -   train_loss = 5.705133036961631
2025-04-11 00:32:14,794 - ***** Epoch: 4: Eval results *****
2025-04-11 00:32:14,795 -   train_loss = 5.06107595231798
2025-04-11 00:33:47,757 - ***** Epoch: 5: Eval results *****
2025-04-11 00:33:47,758 -   train_loss = 4.282909836087908
2025-04-11 00:35:20,832 - ***** Epoch: 6: Eval results *****
2025-04-11 00:35:20,832 -   train_loss = 3.766909012718806
2025-04-11 00:36:54,496 - ***** Epoch: 7: Eval results *****
2025-04-11 00:36:54,497 -   train_loss = 3.4313183928292896
2025-04-11 00:38:27,534 - ***** Epoch: 8: Eval results *****
2025-04-11 00:38:27,534 -   train_loss = 3.217566213910542
2025-04-11 00:40:00,898 - ***** Epoch: 9: Eval results *****
2025-04-11 00:40:00,898 -   train_loss = 3.0877712794712613
2025-04-11 00:41:32,917 - ***** Epoch: 10: Eval results *****
2025-04-11 00:41:32,918 -   train_loss = 2.988344461198837
2025-04-11 00:43:05,379 - ***** Epoch: 11: Eval results *****
2025-04-11 00:43:05,379 -   train_loss = 2.898486508263482
2025-04-11 00:44:38,725 - ***** Epoch: 12: Eval results *****
2025-04-11 00:44:38,726 -   train_loss = 2.8205366929372153
2025-04-11 00:46:11,136 - ***** Epoch: 13: Eval results *****
2025-04-11 00:46:11,137 -   train_loss = 2.7636722155979703
2025-04-11 00:47:44,122 - ***** Epoch: 14: Eval results *****
2025-04-11 00:47:44,122 -   train_loss = 2.7172756422133673
2025-04-11 00:49:17,512 - ***** Epoch: 15: Eval results *****
2025-04-11 00:49:17,513 -   train_loss = 2.680007073614332
2025-04-11 00:50:50,592 - ***** Epoch: 16: Eval results *****
2025-04-11 00:50:50,592 -   train_loss = 2.639928696647523
2025-04-11 00:52:23,709 - ***** Epoch: 17: Eval results *****
2025-04-11 00:52:23,710 -   train_loss = 2.6171871423721313
2025-04-11 00:53:56,573 - ***** Epoch: 18: Eval results *****
2025-04-11 00:53:56,574 -   train_loss = 2.5930672562311567
2025-04-11 00:55:28,984 - ***** Epoch: 19: Eval results *****
2025-04-11 00:55:28,984 -   train_loss = 2.5661547581354776
2025-04-11 00:57:01,082 - ***** Epoch: 20: Eval results *****
2025-04-11 00:57:01,083 -   train_loss = 2.5524571471744113
2025-04-11 00:58:43,115 - ***** Epoch: 21: Eval results *****
2025-04-11 00:58:43,116 -   train_loss = 2.5348567830191717
2025-04-11 01:00:17,215 - ***** Epoch: 22: Eval results *****
2025-04-11 01:00:17,215 -   train_loss = 2.5124952074081177
2025-04-11 01:01:57,134 - ***** Epoch: 23: Eval results *****
2025-04-11 01:01:57,135 -   train_loss = 2.502493629379878
2025-04-11 01:03:30,059 - ***** Epoch: 24: Eval results *****
2025-04-11 01:03:30,060 -   train_loss = 2.483383916673206
2025-04-11 01:04:59,249 - ***** Epoch: 25: Eval results *****
2025-04-11 01:04:59,249 -   train_loss = 2.474950224634201
2025-04-11 01:06:28,394 - ***** Epoch: 26: Eval results *****
2025-04-11 01:06:28,395 -   train_loss = 2.459027048141237
2025-04-11 01:07:58,141 - ***** Epoch: 27: Eval results *****
2025-04-11 01:07:58,141 -   train_loss = 2.4453110297520957
2025-04-11 01:09:26,582 - ***** Epoch: 28: Eval results *****
2025-04-11 01:09:26,582 -   train_loss = 2.440311755452837
2025-04-11 01:10:55,290 - ***** Epoch: 29: Eval results *****
2025-04-11 01:10:55,292 -   train_loss = 2.426521216120039
2025-04-11 01:12:23,234 - ***** Epoch: 30: Eval results *****
2025-04-11 01:12:23,235 -   train_loss = 2.420797172046843
2025-04-11 01:13:52,055 - ***** Epoch: 31: Eval results *****
2025-04-11 01:13:52,056 -   train_loss = 2.4093888070848255
2025-04-11 01:15:20,847 - ***** Epoch: 32: Eval results *****
2025-04-11 01:15:20,848 -   train_loss = 2.4021052633013045
2025-04-11 01:16:48,679 - ***** Epoch: 33: Eval results *****
2025-04-11 01:16:48,680 -   train_loss = 2.395021769735548
2025-04-11 01:18:17,194 - ***** Epoch: 34: Eval results *****
2025-04-11 01:18:17,194 -   train_loss = 2.38335248780629
2025-04-11 01:19:45,293 - ***** Epoch: 35: Eval results *****
2025-04-11 01:19:45,295 -   train_loss = 2.383302650754414
2025-04-11 01:21:12,471 - ***** Epoch: 36: Eval results *****
2025-04-11 01:21:12,472 -   train_loss = 2.3753541821525213
2025-04-11 01:22:40,986 - ***** Epoch: 37: Eval results *****
2025-04-11 01:22:40,986 -   train_loss = 2.370695121704586
2025-04-11 01:24:09,005 - ***** Epoch: 38: Eval results *****
2025-04-11 01:24:09,006 -   train_loss = 2.363607792627244
2025-04-11 01:25:36,948 - ***** Epoch: 39: Eval results *****
2025-04-11 01:25:36,949 -   train_loss = 2.353756003909641
2025-04-11 01:27:04,946 - ***** Epoch: 40: Eval results *****
2025-04-11 01:27:04,948 -   train_loss = 2.3497474931535267
2025-04-11 01:28:32,755 - ***** Epoch: 41: Eval results *****
2025-04-11 01:28:32,756 -   train_loss = 2.346597156827412
2025-04-11 01:30:00,904 - ***** Epoch: 42: Eval results *****
2025-04-11 01:30:00,905 -   train_loss = 2.33887651609996
2025-04-11 01:31:29,097 - ***** Epoch: 43: Eval results *****
2025-04-11 01:31:29,098 -   train_loss = 2.3303951573750328
2025-04-11 01:32:57,830 - ***** Epoch: 44: Eval results *****
2025-04-11 01:32:57,831 -   train_loss = 2.3294965520737665
2025-04-11 01:34:26,349 - ***** Epoch: 45: Eval results *****
2025-04-11 01:34:26,350 -   train_loss = 2.329229474067688
2025-04-11 01:35:54,898 - ***** Epoch: 46: Eval results *****
2025-04-11 01:35:54,899 -   train_loss = 2.3200377547551714
2025-04-11 01:37:22,930 - ***** Epoch: 47: Eval results *****
2025-04-11 01:37:22,931 -   train_loss = 2.320452056233845
2025-04-11 01:38:50,982 - ***** Epoch: 48: Eval results *****
2025-04-11 01:38:50,982 -   train_loss = 2.3161476718054876
2025-04-11 01:40:19,258 - ***** Epoch: 49: Eval results *****
2025-04-11 01:40:19,258 -   train_loss = 2.309887696826269
2025-04-11 01:41:48,038 - ***** Epoch: 50: Eval results *****
2025-04-11 01:41:48,039 -   train_loss = 2.3119705366709877
2025-04-11 01:43:16,570 - ***** Epoch: 51: Eval results *****
2025-04-11 01:43:16,571 -   train_loss = 2.312725309341673
2025-04-11 01:44:44,771 - ***** Epoch: 52: Eval results *****
2025-04-11 01:44:44,772 -   train_loss = 2.298283779431903
2025-04-11 01:46:12,550 - ***** Epoch: 53: Eval results *****
2025-04-11 01:46:12,551 -   train_loss = 2.2940746981000144
2025-04-11 01:47:40,696 - ***** Epoch: 54: Eval results *****
2025-04-11 01:47:40,696 -   train_loss = 2.295258400932191
2025-04-11 01:49:09,001 - ***** Epoch: 55: Eval results *****
2025-04-11 01:49:09,002 -   train_loss = 2.287242020879473
2025-04-11 01:50:36,682 - ***** Epoch: 56: Eval results *****
2025-04-11 01:50:36,684 -   train_loss = 2.2868709431754217
2025-04-11 01:52:04,436 - ***** Epoch: 57: Eval results *****
2025-04-11 01:52:04,437 -   train_loss = 2.288203716278076
2025-04-11 01:53:32,409 - ***** Epoch: 58: Eval results *****
2025-04-11 01:53:32,410 -   train_loss = 2.2849617874811567
2025-04-11 01:54:59,998 - ***** Epoch: 59: Eval results *****
2025-04-11 01:54:59,998 -   train_loss = 2.2798461441009765
2025-04-11 01:56:27,849 - ***** Epoch: 60: Eval results *****
2025-04-11 01:56:27,850 -   train_loss = 2.2829141806042386
2025-04-11 01:57:56,184 - ***** Epoch: 61: Eval results *****
2025-04-11 01:57:56,185 -   train_loss = 2.2743861561729792
2025-04-11 01:59:23,864 - ***** Epoch: 62: Eval results *****
2025-04-11 01:59:23,865 -   train_loss = 2.2724305694065396
2025-04-11 02:00:51,384 - ***** Epoch: 63: Eval results *****
2025-04-11 02:00:51,385 -   train_loss = 2.2722667834115406
2025-04-11 02:02:20,062 - ***** Epoch: 64: Eval results *****
2025-04-11 02:02:20,064 -   train_loss = 2.274521244896783
2025-04-11 02:03:48,685 - ***** Epoch: 65: Eval results *****
2025-04-11 02:03:48,686 -   train_loss = 2.2664315681608898
2025-04-11 02:05:18,320 - ***** Epoch: 66: Eval results *****
2025-04-11 02:05:18,321 -   train_loss = 2.265842222032093
2025-04-11 02:06:48,595 - ***** Epoch: 67: Eval results *****
2025-04-11 02:06:48,597 -   train_loss = 2.26619093191056
2025-04-11 02:08:18,058 - ***** Epoch: 68: Eval results *****
2025-04-11 02:08:18,060 -   train_loss = 2.265288245110285
