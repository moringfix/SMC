2025-04-17 19:36:44,305 - 【半监督】进入了半监督的设定中, 随机抽取标签
2025-04-17 19:36:44,305 - The number of known intents is 5
2025-04-17 19:36:44,305 - Lists of known labels are: [np.str_('Greet'), np.str_('Praise'), np.str_('Ask for help'), np.str_('Joke'), np.str_('Comfort')]
2025-04-17 19:36:44,313 - Number of labeled training samples = 28
2025-04-17 19:36:44,313 - Number of unlabeled training samples = 1306
2025-04-17 19:36:44,313 - Number of evaluation samples = 93
2025-04-17 19:36:44,313 - Number of testing samples = 445
2025-04-17 19:36:44,313 - data preparation...
2025-04-17 19:36:45,866 - data preparation...
2025-04-17 19:36:59,030 - data preparation...
2025-04-17 19:37:00,614 - data preparation...
2025-04-17 19:37:03,231 - num_train_examples = 1334
2025-04-17 19:37:03,232 - ============================== Params ==============================
2025-04-17 19:37:03,232 - logger_name: testmethod_umc_bert-base-uncased_MIntRec_0
2025-04-17 19:37:03,232 - dataset: MIntRec
2025-04-17 19:37:03,232 - multimodal_method: umc
2025-04-17 19:37:03,232 - method: testmethod
2025-04-17 19:37:03,232 - setting: semi_supervised
2025-04-17 19:37:03,232 - merge_dev: False
2025-04-17 19:37:03,232 - text_backbone: bert-base-uncased
2025-04-17 19:37:03,232 - seed: 0
2025-04-17 19:37:03,232 - num_workers: 16
2025-04-17 19:37:03,232 - log_id: testmethod_umc_bert-base-uncased_MIntRec_0_2025-04-17-19-36-44
2025-04-17 19:37:03,232 - gpu_id: 0
2025-04-17 19:37:03,232 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-17 19:37:03,232 - train: True
2025-04-17 19:37:03,232 - tune: True
2025-04-17 19:37:03,232 - save_model: True
2025-04-17 19:37:03,232 - save_results: True
2025-04-17 19:37:03,232 - log_path: logs
2025-04-17 19:37:03,232 - cache_path: cache
2025-04-17 19:37:03,232 - video_data_path: video_data
2025-04-17 19:37:03,232 - audio_data_path: audio_data
2025-04-17 19:37:03,232 - video_feats_path: swin_feats.pkl
2025-04-17 19:37:03,232 - audio_feats_path: wavlm_feats.pkl
2025-04-17 19:37:03,233 - results_path: results
2025-04-17 19:37:03,233 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/SMC/outputs_test
2025-04-17 19:37:03,233 - model_path: models
2025-04-17 19:37:03,233 - config_file_name: testmethod_MIntRec
2025-04-17 19:37:03,233 - results_file_name: results_smc_test.csv
2025-04-17 19:37:03,233 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-17 19:37:03,233 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-17 19:37:03,233 - pretrain_batch_size: 128
2025-04-17 19:37:03,233 - train_batch_size: 128
2025-04-17 19:37:03,233 - eval_batch_size: 64
2025-04-17 19:37:03,233 - test_batch_size: 64
2025-04-17 19:37:03,233 - num_pretrain_epochs: 100
2025-04-17 19:37:03,233 - num_train_epochs: 100
2025-04-17 19:37:03,233 - pretrain: [True]
2025-04-17 19:37:03,233 - aligned_method: ctc
2025-04-17 19:37:03,233 - need_aligned: False
2025-04-17 19:37:03,233 - freeze_pretrain_bert_parameters: [True]
2025-04-17 19:37:03,233 - freeze_train_bert_parameters: [True]
2025-04-17 19:37:03,233 - activation: tanh
2025-04-17 19:37:03,233 - lr_pre: [3e-05]
2025-04-17 19:37:03,233 - lr: [5e-05]
2025-04-17 19:37:03,233 - weight_decay: 0.01
2025-04-17 19:37:03,233 - feat_dim: 768
2025-04-17 19:37:03,233 - hidden_size: 768
2025-04-17 19:37:03,233 - warmup_proportion: 0.1
2025-04-17 19:37:03,233 - hidden_dropout_prob: 0.01
2025-04-17 19:37:03,233 - weight: 1.0
2025-04-17 19:37:03,233 - loss_mode: rdrop
2025-04-17 19:37:03,234 - base_dim: 256
2025-04-17 19:37:03,234 - nheads: 8
2025-04-17 19:37:03,234 - attn_dropout: 0.1
2025-04-17 19:37:03,234 - relu_dropout: 0.01
2025-04-17 19:37:03,234 - embed_dropout: 0.1
2025-04-17 19:37:03,234 - res_dropout: 0.0
2025-04-17 19:37:03,234 - attn_mask: True
2025-04-17 19:37:03,234 - encoder_layers_1: 1
2025-04-17 19:37:03,234 - fusion_act: tanh
2025-04-17 19:37:03,234 - known_cls_ratio: 0.25
2025-04-17 19:37:03,234 - labeled_ratio: 0.1
2025-04-17 19:37:03,234 - cluster_num_factor: 1.0
2025-04-17 19:37:03,234 - wait_patient: 10
2025-04-17 19:37:03,234 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/SMC/outputs_test/testmethod_umc_MIntRec_bert-base-uncased_0
2025-04-17 19:37:03,234 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/SMC/outputs_test/testmethod_umc_MIntRec_bert-base-uncased_0/models
2025-04-17 19:37:03,234 - text_seq_len: 30
2025-04-17 19:37:03,234 - video_seq_len: 230
2025-04-17 19:37:03,234 - audio_seq_len: 480
2025-04-17 19:37:03,234 - text_feat_dim: 768
2025-04-17 19:37:03,234 - video_feat_dim: 1024
2025-04-17 19:37:03,234 - audio_feat_dim: 768
2025-04-17 19:37:03,234 - n_known_cls: 5
2025-04-17 19:37:03,234 - num_labels: 20
2025-04-17 19:37:03,234 - num_train_examples: 1334
2025-04-17 19:37:03,234 - ============================== End Params ==============================
2025-04-17 19:37:04,470 - Freeze all parameters but the last layer for efficiency
2025-04-17 19:37:04,509 - Pre-training start...
2025-04-17 19:37:04,512 - 【测试】【预训练】: 0
2025-04-17 19:37:07,946 - 【测试】【预训练】: tensor([0, 4, 2, 3, 4, 2, 1, 1, 3, 1, 0, 1, 4, 2, 1, 1, 0, 4, 1, 0, 1, 1, 4, 3,
        1, 1, 1, 1], device='cuda:0')
2025-04-17 19:37:13,730 - ***** Epoch: 1: Eval results *****
2025-04-17 19:37:13,730 -   best_score = 0
2025-04-17 19:37:13,730 -   eval_score = 17.2
2025-04-17 19:37:13,730 -   train_loss = 1.6025508642196655
2025-04-17 19:37:13,765 - 【测试】【预训练】: 1
2025-04-17 19:37:17,305 - 【测试】【预训练】: tensor([0, 4, 2, 3, 4, 2, 1, 1, 3, 1, 0, 1, 4, 2, 1, 1, 0, 4, 1, 0, 1, 1, 4, 3,
        1, 1, 1, 1], device='cuda:0')
2025-04-17 19:37:22,209 - ***** Epoch: 2: Eval results *****
2025-04-17 19:37:22,209 -   best_score = 17.2
2025-04-17 19:37:22,209 -   eval_score = 17.2
2025-04-17 19:37:22,209 -   train_loss = 1.6095370054244995
2025-04-17 19:37:22,211 - 【测试】【预训练】: 2
2025-04-17 19:37:25,723 - 【测试】【预训练】: tensor([0, 4, 2, 3, 4, 2, 1, 1, 3, 1, 0, 1, 4, 2, 1, 1, 0, 4, 1, 0, 1, 1, 4, 3,
        1, 1, 1, 1], device='cuda:0')
