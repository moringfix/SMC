2025-04-10 20:40:58,499 - ============================== Params ==============================
2025-04-10 20:40:58,500 - logger_name: umc_umc_bert-base-uncased_MIntRec_0
2025-04-10 20:40:58,500 - dataset: MIntRec
2025-04-10 20:40:58,500 - multimodal_method: umc
2025-04-10 20:40:58,500 - method: umc
2025-04-10 20:40:58,500 - text_backbone: bert-base-uncased
2025-04-10 20:40:58,500 - seed: 0
2025-04-10 20:40:58,500 - num_workers: 16
2025-04-10 20:40:58,500 - log_id: umc_umc_bert-base-uncased_MIntRec_0_2025-04-10-20-40-58
2025-04-10 20:40:58,500 - gpu_id: 0
2025-04-10 20:40:58,500 - data_path: /root/autodl-tmp/home/Share/Dataset/LZH
2025-04-10 20:40:58,500 - train: True
2025-04-10 20:40:58,500 - tune: True
2025-04-10 20:40:58,500 - save_model: True
2025-04-10 20:40:58,500 - save_results: True
2025-04-10 20:40:58,500 - log_path: logs
2025-04-10 20:40:58,500 - cache_path: cache
2025-04-10 20:40:58,500 - video_data_path: video_data
2025-04-10 20:40:58,500 - audio_data_path: audio_data
2025-04-10 20:40:58,500 - video_feats_path: swin_feats.pkl
2025-04-10 20:40:58,500 - audio_feats_path: wavlm_feats.pkl
2025-04-10 20:40:58,500 - results_path: results
2025-04-10 20:40:58,500 - output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec
2025-04-10 20:40:58,500 - model_path: models
2025-04-10 20:40:58,500 - config_file_name: umc_MIntRec
2025-04-10 20:40:58,500 - results_file_name: results_umc.csv
2025-04-10 20:40:58,500 - text_pretrained_model: uncased_L-12_H-768_A-12
2025-04-10 20:40:58,500 - text_seq_len: 30
2025-04-10 20:40:58,500 - video_seq_len: 230
2025-04-10 20:40:58,501 - audio_seq_len: 480
2025-04-10 20:40:58,501 - text_feat_dim: 768
2025-04-10 20:40:58,501 - video_feat_dim: 1024
2025-04-10 20:40:58,501 - audio_feat_dim: 768
2025-04-10 20:40:58,501 - num_labels: 20
2025-04-10 20:40:58,501 - num_train_examples: 1779
2025-04-10 20:40:58,501 - pretrained_bert_model: uncased_L-12_H-768_A-12
2025-04-10 20:40:58,501 - pretrain_batch_size: 128
2025-04-10 20:40:58,501 - train_batch_size: 128
2025-04-10 20:40:58,501 - eval_batch_size: 128
2025-04-10 20:40:58,501 - test_batch_size: 128
2025-04-10 20:40:58,501 - num_pretrain_epochs: 100
2025-04-10 20:40:58,501 - num_train_epochs: 100
2025-04-10 20:40:58,501 - pretrain: [True]
2025-04-10 20:40:58,501 - aligned_method: ctc
2025-04-10 20:40:58,501 - need_aligned: False
2025-04-10 20:40:58,501 - freeze_pretrain_bert_parameters: [True]
2025-04-10 20:40:58,501 - freeze_train_bert_parameters: [True]
2025-04-10 20:40:58,501 - pretrain_temperature: [0.2]
2025-04-10 20:40:58,501 - train_temperature_sup: [1.4]
2025-04-10 20:40:58,501 - train_temperature_unsup: [1]
2025-04-10 20:40:58,501 - activation: tanh
2025-04-10 20:40:58,501 - lr_pre: 2e-05
2025-04-10 20:40:58,501 - lr: [0.0003]
2025-04-10 20:40:58,501 - delta: [0.05]
2025-04-10 20:40:58,501 - thres: [0.1]
2025-04-10 20:40:58,501 - topk: [5]
2025-04-10 20:40:58,501 - weight_decay: 0.01
2025-04-10 20:40:58,501 - feat_dim: 768
2025-04-10 20:40:58,501 - hidden_size: 768
2025-04-10 20:40:58,501 - grad_clip: -1.0
2025-04-10 20:40:58,502 - warmup_proportion: 0.1
2025-04-10 20:40:58,502 - hidden_dropout_prob: 0.1
2025-04-10 20:40:58,502 - weight: 1.0
2025-04-10 20:40:58,502 - loss_mode: rdrop
2025-04-10 20:40:58,502 - base_dim: 256
2025-04-10 20:40:58,502 - nheads: 8
2025-04-10 20:40:58,502 - attn_dropout: 0.1
2025-04-10 20:40:58,502 - relu_dropout: 0.1
2025-04-10 20:40:58,502 - embed_dropout: 0.1
2025-04-10 20:40:58,502 - res_dropout: 0.0
2025-04-10 20:40:58,502 - attn_mask: True
2025-04-10 20:40:58,502 - encoder_layers_1: 1
2025-04-10 20:40:58,502 - fusion_act: tanh
2025-04-10 20:40:58,502 - pred_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec/umc_umc_MIntRec_bert-base-uncased_0
2025-04-10 20:40:58,502 - model_output_path: /root/autodl-tmp/home/lizhuohang/reaserch/EMNLP/UMC/outputs/MIntRec/umc_umc_MIntRec_bert-base-uncased_0/models
2025-04-10 20:40:58,502 - ============================== End Params ==============================
2025-04-10 20:40:59,541 - Freeze all parameters but the last layer for efficiency
2025-04-10 20:40:59,580 - Pre-training start...
2025-04-10 20:41:13,588 - ***** Epoch: 1: Eval results *****
2025-04-10 20:41:13,588 -   train_loss = 5.938369989395142
2025-04-10 20:41:27,050 - ***** Epoch: 2: Eval results *****
2025-04-10 20:41:27,051 -   train_loss = 5.937172821589878
2025-04-10 20:41:40,658 - ***** Epoch: 3: Eval results *****
2025-04-10 20:41:40,658 -   train_loss = 5.931615965706961
2025-04-10 20:41:55,347 - ***** Epoch: 4: Eval results *****
2025-04-10 20:41:55,347 -   train_loss = 5.927084684371948
2025-04-10 20:42:09,872 - ***** Epoch: 5: Eval results *****
2025-04-10 20:42:09,872 -   train_loss = 5.910839421408517
2025-04-10 20:42:24,462 - ***** Epoch: 6: Eval results *****
2025-04-10 20:42:24,462 -   train_loss = 5.885191440582275
2025-04-10 20:42:40,669 - ***** Epoch: 7: Eval results *****
2025-04-10 20:42:40,670 -   train_loss = 5.7869745663234164
2025-04-10 20:42:57,049 - ***** Epoch: 8: Eval results *****
2025-04-10 20:42:57,049 -   train_loss = 5.513078451156616
2025-04-10 20:43:13,022 - ***** Epoch: 9: Eval results *****
2025-04-10 20:43:13,023 -   train_loss = 5.044661998748779
2025-04-10 20:43:29,072 - ***** Epoch: 10: Eval results *****
2025-04-10 20:43:29,072 -   train_loss = 4.582355771745954
2025-04-10 20:43:45,006 - ***** Epoch: 11: Eval results *****
2025-04-10 20:43:45,007 -   train_loss = 4.232972213200161
2025-04-10 20:44:00,879 - ***** Epoch: 12: Eval results *****
2025-04-10 20:44:00,879 -   train_loss = 3.9741884810583934
2025-04-10 20:44:16,857 - ***** Epoch: 13: Eval results *****
2025-04-10 20:44:16,858 -   train_loss = 3.773118359701974
2025-04-10 20:44:32,878 - ***** Epoch: 14: Eval results *****
2025-04-10 20:44:32,878 -   train_loss = 3.610594187464033
2025-04-10 20:44:48,470 - ***** Epoch: 15: Eval results *****
2025-04-10 20:44:48,470 -   train_loss = 3.4917663676398143
2025-04-10 20:45:04,280 - ***** Epoch: 16: Eval results *****
2025-04-10 20:45:04,280 -   train_loss = 3.388807398932321
2025-04-10 20:45:19,861 - ***** Epoch: 17: Eval results *****
2025-04-10 20:45:19,861 -   train_loss = 3.333966476576669
2025-04-10 20:45:34,813 - ***** Epoch: 18: Eval results *****
2025-04-10 20:45:34,814 -   train_loss = 3.2792770862579346
2025-04-10 20:45:49,662 - ***** Epoch: 19: Eval results *****
2025-04-10 20:45:49,662 -   train_loss = 3.244722740990775
2025-04-10 20:46:04,248 - ***** Epoch: 20: Eval results *****
2025-04-10 20:46:04,249 -   train_loss = 3.1968275819505965
2025-04-10 20:46:19,300 - ***** Epoch: 21: Eval results *****
2025-04-10 20:46:19,300 -   train_loss = 3.1681395939418246
2025-04-10 20:46:33,832 - ***** Epoch: 22: Eval results *****
2025-04-10 20:46:33,833 -   train_loss = 3.1358038187026978
2025-04-10 20:46:48,442 - ***** Epoch: 23: Eval results *****
2025-04-10 20:46:48,442 -   train_loss = 3.107098545346941
2025-04-10 20:47:03,288 - ***** Epoch: 24: Eval results *****
2025-04-10 20:47:03,288 -   train_loss = 3.0684227091925487
2025-04-10 20:47:18,713 - ***** Epoch: 25: Eval results *****
2025-04-10 20:47:18,714 -   train_loss = 3.051334261894226
2025-04-10 20:47:33,356 - ***** Epoch: 26: Eval results *****
2025-04-10 20:47:33,356 -   train_loss = 3.027230279786246
2025-04-10 20:47:48,390 - ***** Epoch: 27: Eval results *****
2025-04-10 20:47:48,391 -   train_loss = 3.0187735898154124
2025-04-10 20:48:03,497 - ***** Epoch: 28: Eval results *****
2025-04-10 20:48:03,498 -   train_loss = 2.985887118748256
2025-04-10 20:48:18,138 - ***** Epoch: 29: Eval results *****
2025-04-10 20:48:18,138 -   train_loss = 2.9730291025979176
2025-04-10 20:48:32,635 - ***** Epoch: 30: Eval results *****
2025-04-10 20:48:32,635 -   train_loss = 2.9463283675057546
2025-04-10 20:48:47,818 - ***** Epoch: 31: Eval results *****
2025-04-10 20:48:47,819 -   train_loss = 2.929747428212847
2025-04-10 20:49:02,729 - ***** Epoch: 32: Eval results *****
2025-04-10 20:49:02,729 -   train_loss = 2.923645496368408
2025-04-10 20:49:16,964 - ***** Epoch: 33: Eval results *****
2025-04-10 20:49:16,964 -   train_loss = 2.904875193323408
2025-04-10 20:49:30,827 - ***** Epoch: 34: Eval results *****
2025-04-10 20:49:30,827 -   train_loss = 2.8894005673272267
2025-04-10 20:49:44,943 - ***** Epoch: 35: Eval results *****
2025-04-10 20:49:44,943 -   train_loss = 2.889553887503488
2025-04-10 20:49:59,167 - ***** Epoch: 36: Eval results *****
2025-04-10 20:49:59,167 -   train_loss = 2.858026691845485
2025-04-10 20:50:13,379 - ***** Epoch: 37: Eval results *****
2025-04-10 20:50:13,380 -   train_loss = 2.84201671395983
2025-04-10 20:50:28,081 - ***** Epoch: 38: Eval results *****
2025-04-10 20:50:28,082 -   train_loss = 2.8305706637246266
2025-04-10 20:50:42,053 - ***** Epoch: 39: Eval results *****
2025-04-10 20:50:42,054 -   train_loss = 2.830872212137495
2025-04-10 20:50:56,065 - ***** Epoch: 40: Eval results *****
2025-04-10 20:50:56,065 -   train_loss = 2.820229802812849
2025-04-10 20:51:10,147 - ***** Epoch: 41: Eval results *****
2025-04-10 20:51:10,148 -   train_loss = 2.813401460647583
2025-04-10 20:51:24,351 - ***** Epoch: 42: Eval results *****
2025-04-10 20:51:24,352 -   train_loss = 2.810617055211748
2025-04-10 20:51:38,684 - ***** Epoch: 43: Eval results *****
2025-04-10 20:51:38,684 -   train_loss = 2.791488153593881
2025-04-10 20:51:53,078 - ***** Epoch: 44: Eval results *****
2025-04-10 20:51:53,079 -   train_loss = 2.7886240652629306
2025-04-10 20:52:07,670 - ***** Epoch: 45: Eval results *****
2025-04-10 20:52:07,670 -   train_loss = 2.7831132411956787
2025-04-10 20:52:21,654 - ***** Epoch: 46: Eval results *****
2025-04-10 20:52:21,654 -   train_loss = 2.7711129869733537
2025-04-10 20:52:35,649 - ***** Epoch: 47: Eval results *****
2025-04-10 20:52:35,649 -   train_loss = 2.7657303639820645
2025-04-10 20:52:49,738 - ***** Epoch: 48: Eval results *****
2025-04-10 20:52:49,738 -   train_loss = 2.759146281651088
2025-04-10 20:53:03,784 - ***** Epoch: 49: Eval results *****
2025-04-10 20:53:03,784 -   train_loss = 2.743150387491499
2025-04-10 20:53:17,609 - ***** Epoch: 50: Eval results *****
2025-04-10 20:53:17,609 -   train_loss = 2.7392477818897794
2025-04-10 20:53:31,813 - ***** Epoch: 51: Eval results *****
2025-04-10 20:53:31,814 -   train_loss = 2.751292518207005
2025-04-10 20:53:46,244 - ***** Epoch: 52: Eval results *****
2025-04-10 20:53:46,244 -   train_loss = 2.732565539223807
2025-04-10 20:54:00,371 - ***** Epoch: 53: Eval results *****
2025-04-10 20:54:00,372 -   train_loss = 2.7264172860554288
2025-04-10 20:54:14,610 - ***** Epoch: 54: Eval results *****
2025-04-10 20:54:14,610 -   train_loss = 2.7336704901286533
2025-04-10 20:54:28,871 - ***** Epoch: 55: Eval results *****
2025-04-10 20:54:28,871 -   train_loss = 2.728749769074576
2025-04-10 20:54:43,244 - ***** Epoch: 56: Eval results *****
2025-04-10 20:54:43,244 -   train_loss = 2.711954321180071
2025-04-10 20:54:57,319 - ***** Epoch: 57: Eval results *****
2025-04-10 20:54:57,319 -   train_loss = 2.7110101836068288
2025-04-10 20:55:11,152 - ***** Epoch: 58: Eval results *****
2025-04-10 20:55:11,153 -   train_loss = 2.701969248907907
2025-04-10 20:55:25,314 - ***** Epoch: 59: Eval results *****
2025-04-10 20:55:25,314 -   train_loss = 2.7003055810928345
2025-04-10 20:55:40,509 - ***** Epoch: 60: Eval results *****
2025-04-10 20:55:40,510 -   train_loss = 2.690510834966387
2025-04-10 20:55:56,546 - ***** Epoch: 61: Eval results *****
2025-04-10 20:55:56,546 -   train_loss = 2.6906602723257884
2025-04-10 20:56:12,173 - ***** Epoch: 62: Eval results *****
2025-04-10 20:56:12,173 -   train_loss = 2.6836135046822682
2025-04-10 20:56:27,296 - ***** Epoch: 63: Eval results *****
2025-04-10 20:56:27,296 -   train_loss = 2.6908436843327115
2025-04-10 20:56:41,791 - ***** Epoch: 64: Eval results *****
2025-04-10 20:56:41,792 -   train_loss = 2.6694948502949307
2025-04-10 20:56:56,059 - ***** Epoch: 65: Eval results *****
2025-04-10 20:56:56,059 -   train_loss = 2.676017642021179
2025-04-10 20:57:09,841 - ***** Epoch: 66: Eval results *****
2025-04-10 20:57:09,841 -   train_loss = 2.6760528768811906
2025-04-10 20:57:23,573 - ***** Epoch: 67: Eval results *****
2025-04-10 20:57:23,573 -   train_loss = 2.6579098871776035
2025-04-10 20:57:37,021 - ***** Epoch: 68: Eval results *****
2025-04-10 20:57:37,021 -   train_loss = 2.6642896958759854
2025-04-10 20:57:50,937 - ***** Epoch: 69: Eval results *****
2025-04-10 20:57:50,937 -   train_loss = 2.655852505138942
2025-04-10 20:58:04,749 - ***** Epoch: 70: Eval results *****
2025-04-10 20:58:04,750 -   train_loss = 2.6657282454626903
2025-04-10 20:58:18,287 - ***** Epoch: 71: Eval results *****
2025-04-10 20:58:18,287 -   train_loss = 2.6649221863065446
2025-04-10 20:58:31,968 - ***** Epoch: 72: Eval results *****
2025-04-10 20:58:31,968 -   train_loss = 2.6623952388763428
2025-04-10 20:58:45,745 - ***** Epoch: 73: Eval results *****
2025-04-10 20:58:45,745 -   train_loss = 2.664837292262486
2025-04-10 20:58:59,464 - ***** Epoch: 74: Eval results *****
2025-04-10 20:58:59,465 -   train_loss = 2.6617584569113597
2025-04-10 20:59:13,036 - ***** Epoch: 75: Eval results *****
2025-04-10 20:59:13,037 -   train_loss = 2.664836662156241
2025-04-10 20:59:27,351 - ***** Epoch: 76: Eval results *****
2025-04-10 20:59:27,351 -   train_loss = 2.6476623671395436
2025-04-10 20:59:41,420 - ***** Epoch: 77: Eval results *****
2025-04-10 20:59:41,421 -   train_loss = 2.642677528517587
2025-04-10 20:59:55,459 - ***** Epoch: 78: Eval results *****
2025-04-10 20:59:55,459 -   train_loss = 2.6402431896754672
2025-04-10 21:00:09,462 - ***** Epoch: 79: Eval results *****
2025-04-10 21:00:09,462 -   train_loss = 2.643846767289298
2025-04-10 21:00:23,250 - ***** Epoch: 80: Eval results *****
2025-04-10 21:00:23,250 -   train_loss = 2.6425416128976003
2025-04-10 21:00:36,918 - ***** Epoch: 81: Eval results *****
2025-04-10 21:00:36,918 -   train_loss = 2.6435724837439403
2025-04-10 21:00:50,528 - ***** Epoch: 82: Eval results *****
2025-04-10 21:00:50,528 -   train_loss = 2.640886970928737
2025-04-10 21:01:04,720 - ***** Epoch: 83: Eval results *****
2025-04-10 21:01:04,721 -   train_loss = 2.6530385528291975
2025-04-10 21:01:18,692 - ***** Epoch: 84: Eval results *****
2025-04-10 21:01:18,692 -   train_loss = 2.6379246711730957
2025-04-10 21:01:32,437 - ***** Epoch: 85: Eval results *****
2025-04-10 21:01:32,438 -   train_loss = 2.6386716025216237
2025-04-10 21:01:46,364 - ***** Epoch: 86: Eval results *****
2025-04-10 21:01:46,364 -   train_loss = 2.6378811427525113
2025-04-10 21:02:00,123 - ***** Epoch: 87: Eval results *****
2025-04-10 21:02:00,124 -   train_loss = 2.634073717253549
2025-04-10 21:02:14,166 - ***** Epoch: 88: Eval results *****
2025-04-10 21:02:14,166 -   train_loss = 2.6373197180884227
2025-04-10 21:02:28,358 - ***** Epoch: 89: Eval results *****
2025-04-10 21:02:28,359 -   train_loss = 2.632757919175284
2025-04-10 21:02:42,862 - ***** Epoch: 90: Eval results *****
2025-04-10 21:02:42,863 -   train_loss = 2.6364808763776506
2025-04-10 21:02:56,446 - ***** Epoch: 91: Eval results *****
2025-04-10 21:02:56,446 -   train_loss = 2.6315796204975674
2025-04-10 21:03:10,344 - ***** Epoch: 92: Eval results *****
2025-04-10 21:03:10,344 -   train_loss = 2.629815902028765
2025-04-10 21:03:24,137 - ***** Epoch: 93: Eval results *****
2025-04-10 21:03:24,138 -   train_loss = 2.6351203407560075
2025-04-10 21:03:37,962 - ***** Epoch: 94: Eval results *****
2025-04-10 21:03:37,962 -   train_loss = 2.634196298463004
2025-04-10 21:03:52,244 - ***** Epoch: 95: Eval results *****
2025-04-10 21:03:52,244 -   train_loss = 2.632374269621713
2025-04-10 21:04:06,130 - ***** Epoch: 96: Eval results *****
2025-04-10 21:04:06,130 -   train_loss = 2.6321056400026595
2025-04-10 21:04:19,936 - ***** Epoch: 97: Eval results *****
2025-04-10 21:04:19,936 -   train_loss = 2.635944894381932
2025-04-10 21:04:33,832 - ***** Epoch: 98: Eval results *****
2025-04-10 21:04:33,832 -   train_loss = 2.644279956817627
2025-04-10 21:04:48,193 - ***** Epoch: 99: Eval results *****
2025-04-10 21:04:48,194 -   train_loss = 2.6299379553113664
2025-04-10 21:05:02,400 - ***** Epoch: 100: Eval results *****
2025-04-10 21:05:02,400 -   train_loss = 2.631145102637155
2025-04-10 21:05:02,896 - Pre-training finished...
2025-04-10 21:05:03,149 - Freeze all parameters but the last layer for efficiency
2025-04-10 21:05:03,157 - Multimodal Intent Recognition begins...
2025-04-10 21:05:03,158 - Training begins...
2025-04-10 21:05:19,739 - Initializing centroids with K-means++...
